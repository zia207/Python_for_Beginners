{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/zia207/Python_for_Beginners/blob/main/Notebook/01_01_07_data_import_export_python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KSwGRytJdppk"
      },
      "source": [
        "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YbHLFtuSdppk"
      },
      "source": [
        "# 1.4 Data Import/Export in Python\n",
        "\n",
        "This tutorial provides a comprehensive overview of how to import and export data in Python, covering various file formats such as CSV, Excel, JSON, and more. It also includes practical examples and code snippets to help you understand the process better. By the end of this tutorial, you will have a solid understanding of how to work with data in Python and be able to apply these techniques in your own projects."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BiaF1laqdppl"
      },
      "source": [
        "## Introduction\n",
        "\n",
        "One of the most important steps in data analysis is importing data into Python and exporting data from Python. This process can be done using various functions and libraries depending on the format of the data, such as CSV, Excel, or files from statistical software like SPSS or Stata. Mastering data import and export is a fundamental skill for any data scientist or analyst, as it allows you to work with real-world datasets and share your results effectively."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HkCHHiZzdppl"
      },
      "source": [
        "### Check and Install Required Python Packages\n",
        "\n",
        "In this exercise, we will use the following Python libraries:\n",
        "\n",
        "1.  **pandas**: The primary library for data manipulation and analysis. It can read and write CSV, Excel, and JSON files.\n",
        "2.  **openpyxl**: A dependency for `pandas` to read and write modern Excel (`.xlsx`) files.\n",
        "3.  **pyreadstat**: To read and write data from other statistical software like SPSS, Stata, and SAS. This is the Python equivalent of R's `haven` and `foreign` packages.\n",
        "4.  **json**: A built-in Python module for handling JSON data."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "oztHcgaFdppm",
        "outputId": "b63c94e7-2648-4155-9d50-06ac5dab5321"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "'pandas' is already installed.\n",
            "'openpyxl' is already installed.\n",
            "'pyreadstat' is not installed. Installing now...\n",
            "Defaulting to user installation because normal site-packages is not writeable\n",
            "Collecting pyreadstat\n",
            "  Using cached pyreadstat-1.3.1-cp310-cp310-manylinux2014_x86_64.manylinux_2_17_x86_64.whl (633 kB)\n",
            "Collecting narwhals>=2.0\n",
            "  Using cached narwhals-2.5.0-py3-none-any.whl (407 kB)\n",
            "Requirement already satisfied: numpy in /home/zia207/.local/lib/python3.10/site-packages (from pyreadstat) (1.26.4)\n",
            "Installing collected packages: narwhals, pyreadstat\n",
            "Successfully installed narwhals-2.5.0 pyreadstat-1.3.1\n",
            "\n",
            " All required packages are installed.\n"
          ]
        }
      ],
      "source": [
        "# List of required packages\n",
        "required_packages = [\n",
        "    'pandas',\n",
        "    'openpyxl',   # For Excel .xlsx files\n",
        "    'pyreadstat'  # For SPSS, Stata, SAS files\n",
        "    # 'json' is built-in, so no need to install\n",
        "]\n",
        "\n",
        "import importlib\n",
        "import subprocess\n",
        "import sys\n",
        "\n",
        "def install_package(package):\n",
        "    \"\"\"Install a package using pip.\"\"\"\n",
        "    subprocess.check_call([sys.executable, \"-m\", \"pip\", \"install\", package])\n",
        "\n",
        "# Check and install missing packages\n",
        "for package in required_packages:\n",
        "    try:\n",
        "        importlib.import_module(package)\n",
        "        print(f\"'{package}' is already installed.\")\n",
        "    except ImportError:\n",
        "        print(f\"'{package}' is not installed. Installing now...\")\n",
        "        install_package(package)\n",
        "\n",
        "print(\"\\n All required packages are installed.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "sSl7DTIDdppo"
      },
      "source": [
        "### Import Python Libraries\n",
        "\n",
        "Now that the packages are installed, we import them for use."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "CvbwTcdwdppo",
        "outputId": "69c68f15-ce30-4e80-8252-10f44725dbbf"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Libraries successfully imported.\n"
          ]
        }
      ],
      "source": [
        "# Import the necessary libraries\n",
        "import pandas as pd\n",
        "import json\n",
        "import pyreadstat\n",
        "import os\n",
        "\n",
        "print(\"Libraries successfully imported.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "f_ugm0iBdppp"
      },
      "source": [
        "### Set Working Directory and Define Data Paths\n",
        "\n",
        "Before we start, it's good practice to set or check your working directory. This is where Python will look for files to read and where it will save files by default."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "X7IKxqZedppp",
        "outputId": "b16c5600-1b88-42d9-d486-3ed5af82e77e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Current Working Directory:\n",
            "/home/zia207/Dropbox/WebSites/GitHub_repository/python-websites/Python_for_Beginners/Notebook\n",
            "\n",
            "Files in '/home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/':\n",
            "['LBC_Data_PM25.csv', 'nepal_df_meta_data.csv', 'rice_data.dta', 'LBC_Data.csv', 'rice_data.sav', '03_poverty_2004_2016_both.csv', 'data_2004_2016_long_diabestes.csv', 'rice_data.rds', 'test_data.json', 'DT.csv', 'output.csv', 'data_2004.csv', 'diabetes_dignosed_2004_2016_total.csv', 'rice_data.xlsx', 'nepal_df_balance.csv', 'test_data.xpt', 'test_data.sav', 'nyc-taxi-tiny.zip', 'LBC_data.feather', 'taxi-zone-lookup.csv', '06_uninsured_2012_2016.csv', 'data_2009.csv', 'taxi_zone_lookup.csv', 'data_2005.csv', 'LBC_Data_ID.csv', 'data_2008.csv', 'test_data.txt', 'gp_soil_data.csv', 'rice_data.csv', 'test_data.sas7bdat', 'data_2001.csv', 'data_2002.csv', 'data_2006.csv', 'napal_data.feather', 'data_2007.csv', 'df.chem_02.csv', 'rice_data.RData', 'rice_data.json', '04_education_2012_2016.csv', 'greet.py', 'napal_data.parquet', 'PAHdata.csv', 'FIPS_CODE.csv', 'rice_arsenic_data.csv', 'test_data.xlsx', 'usa_county.csv', 'USA_LBC_Data_raw.csv', 'rice_data.pkl', 'LBC_Data_Smoking.csv', '__pycache__', '01_obesity_2004_2016_both.csv', 'gp_soil_data_syn.csv', 'multi_objects.pkl', 'gp_soil_data_na.csv', 'LBC_Data_Rate.csv', '02_physical_inactivity_2004_2016.csv', 'data_2010.csv', 'data_2003.csv', 'test_data.dta', 'nyc-taxi-tiny', 'usa_geochemical_raw.csv', 'rice_data.sas7bdat', 'usa_state.csv', 'df.csv', 'usa_division.csv', 'data_2000.csv', 'usa_corn_production.csv', 'data_2011.csv', 'test_data.csv', 'data_1999.csv', '05_food_env_index_2014_2016.csv', 'diabetes_dignosed_2004_2016l.csv', 'yellow_tripdata_2023.parquet', 'data_1998_2010_long_lbc.csv', 'data_1998.csv', 'data_2012.csv']\n"
          ]
        }
      ],
      "source": [
        "# Check current working directory\n",
        "print(\"Current Working Directory:\")\n",
        "print(os.getcwd())\n",
        "\n",
        "# Define the data folder path\n",
        "# Replace this with the path to your local data folder\n",
        "data_folder = \"/home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/\"\n",
        "\n",
        "# If you want to set this as your working directory, uncomment the line below:\n",
        "os.chdir(data_folder)\n",
        "\n",
        "# List files in the data directory (optional)\n",
        "try:\n",
        "    print(f\"\\nFiles in '{data_folder}':\")\n",
        "    print(os.listdir(data_folder))\n",
        "except FileNotFoundError:\n",
        "    print(f\"\\n  The directory '{data_folder}' does not exist. Using GitHub URLs for data.\")\n",
        "\n",
        "# For this tutorial, we will primarily use direct URLs from GitHub to ensure the code runs anywhere.\n",
        "github_data_url = \"https://github.com/zia207/Python_for_Beginners/tree/main/Data/\"\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "D8Q5dQzTdppp"
      },
      "source": [
        "## Data Import Into Python\n",
        "\n",
        "Data importing is the process of reading data from external files or databases into Python for analysis. Python, with its rich ecosystem of libraries, makes this process straightforward for a wide variety of file formats."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B0mgXoxsdppp"
      },
      "source": [
        "### Read Text File (.txt)\n",
        "\n",
        "A text file (`.txt`) contains plain text. In data science, these are often delimited files (e.g., tab-separated or space-separated). We use `pandas.read_csv()` for this, specifying the appropriate delimiter."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5HYnmILLdppq",
        "outputId": "8f362e8e-dd96-4531-e05f-821b5d191f89"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n",
            "['ID', 'treat', 'var', 'rep', 'PH', 'TN', 'PN', 'GW', 'ster', 'DTM', 'SW', 'GAs', 'STAs']\n"
          ]
        }
      ],
      "source": [
        "# Read a .txt file (assuming it's tab or space-delimited)\n",
        "# If it's space-delimited, use sep='\\s+'\n",
        "df_txt = pd.read_csv(\n",
        "    os.path.join(data_folder, \"test_data.txt\"),\n",
        "    sep='\\t',  # Change this to ' ' or ',' as needed\n",
        "    header=0   # Use the first row as column names\n",
        ")\n",
        "\n",
        "# Or read directly from GitHub\n",
        "df_txt = pd.read_csv(\n",
        "    \"https://github.com/zia207/Python_for_Beginners/raw/refs/heads/main/Data/test_data.txt\",\n",
        "    sep='\\t',  # Adjust delimiter\n",
        "    header=0\n",
        ")\n",
        "\n",
        "print(df_txt.head())\n",
        "print(df_txt.columns.tolist())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "skTk1qGedppq"
      },
      "source": [
        "If you want to set `data_folder` as your working directory, read data from there:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eyEBHSoGdppq",
        "outputId": "1475a2aa-14a7-4c59-f786-cf261fdf63e4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n"
          ]
        }
      ],
      "source": [
        "# If you want to set data_folder as your working directory,\n",
        "os.chdir(data_folder)\n",
        "df_txt = pd.read_csv(\"test_data.txt\", sep='\\t')\n",
        "print(df_txt.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ih_F5oEbdppq"
      },
      "source": [
        "### Read Comma-Separated File (.csv)\n",
        "\n",
        "A CSV (Comma-Separated Values) file is one of the most common formats for data exchange. The `pandas.read_csv()` function is designed for this."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sIQ9q47Tdppq",
        "outputId": "21c2aff8-3dff-42e6-d03e-b9b8b352f565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n"
          ]
        }
      ],
      "source": [
        "# Read a .csv file\n",
        "df_csv = pd.read_csv(\n",
        "    os.path.join(data_folder, \"test_data.csv\")\n",
        ")\n",
        "\n",
        "# Or from GitHub\n",
        "df_csv = pd.read_csv(\n",
        "    \"https://github.com/zia207/Python_for_Beginners/raw/refs/heads/main/Data/test_data.csv\"\n",
        ")\n",
        "print(df_csv.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "dGdB_G80dppr"
      },
      "source": [
        "### Read Excel Files (.xlsx, .xls)\n",
        "\n",
        "To read Excel files, we use `pandas.read_excel()`. This function can handle both the modern `.xlsx` format and the legacy `.xls` format."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IaYT1gYFdppr",
        "outputId": "dc38d23c-4707-4ada-8b1b-6298f3950b33"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not read Excel file from URL: Excel file format cannot be determined, you must specify an engine manually.\n",
            "Please download the file locally and adjust the path.\n"
          ]
        }
      ],
      "source": [
        "# Read an Excel file from your local directory or from GitHub\n",
        "# Note: Reading .xlsx files directly from a URL can sometimes be unreliable.\n",
        "# For local files, use: pd.read_excel(os.path.join(data_folder, \"test_data.xlsx\"))\n",
        "\n",
        "# Read an Excel file\n",
        "df_xl = pd.read_excel(\n",
        "    os.path.join(data_folder, \"test_data.xlsx\"),\n",
        "    sheet_name=0  # Read the first sheet (0-indexed)\n",
        ")\n",
        "\n",
        "\n",
        "# or\n",
        "\n",
        "try:\n",
        "    df_xl = pd.read_excel(\n",
        "        github_data_url + \"test_data.xlsx\",\n",
        "        sheet_name=0  # Read the first sheet\n",
        "    )\n",
        "    print(\"First few rows of the Excel file:\")\n",
        "    print(df_xl.head())\n",
        "except Exception as e:\n",
        "    print(f\"Could not read Excel file from URL: {e}\")\n",
        "    print(\"Please download the file locally and adjust the path.\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "A6DvTQwMdppr"
      },
      "source": [
        "### Read JSON Files (.json)\n",
        "\n",
        "JSON (JavaScript Object Notation) is a lightweight data-interchange format. It is easy for humans to read and write and easy for machines to parse and generate. In Python, we can read it using `pandas.read_json()` or the built-in `json` module."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lGof4DCudppr",
        "outputId": "f5b26ad3-93e1-4834-ee1d-fc0b42e1d1c0"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Could not read JSON file with pandas: Expected object or value\n",
            "Could not read JSON file with json module: Expecting value: line 7 column 1 (char 6)\n"
          ]
        }
      ],
      "source": [
        "# Method 1: Using pandas (if the JSON is a simple array of objects)\n",
        "try:\n",
        "    df_json = pd.read_json(\n",
        "        github_data_url + \"test_data.json\"\n",
        "    )\n",
        "    print(\"First few rows of the JSON file (via pandas):\")\n",
        "    print(df_json.head())\n",
        "except Exception as e:\n",
        "    print(f\"Could not read JSON file with pandas: {e}\")\n",
        "\n",
        "# Method 2: Using the json module (more flexible for complex structures)\n",
        "import urllib.request\n",
        "import json\n",
        "\n",
        "try:\n",
        "    with urllib.request.urlopen(github_data_url + \"test_data.json\") as url:\n",
        "        json_data = json.load(url)\n",
        "\n",
        "    # Convert to DataFrame if it's a suitable structure\n",
        "    df_json = pd.DataFrame(json_data)\n",
        "    print(\"\\nFirst few rows of the JSON file (via json module):\")\n",
        "    print(df_json.head())\n",
        "\n",
        "except Exception as e:\n",
        "    print(f\"Could not read JSON file with json module: {e}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qxwMvLuEdpps"
      },
      "source": [
        "### Import Data from Other Statistical Software\n",
        "\n",
        "The `pyreadstat` library allows Python to read and write data formats from statistical software like SPSS, Stata, and SAS. It is the direct counterpart to R's `haven` package."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "51D0OPLsdpps"
      },
      "source": [
        "#### Read STATA File (.dta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "GZPeF6Dkdpps",
        "outputId": "330495f3-b0dd-4a73-c267-748b0b32e931"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the STATA file:\n",
            "   ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0   1  Low As  BR01    1   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1   2  Low As  BR01    2  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2   3  Low As  BR01    3  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3   4  Low As  BR06    1  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4   5  Low As  BR06    2  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n"
          ]
        }
      ],
      "source": [
        "df_dta, meta_dta = pyreadstat.read_dta(\n",
        "    os.path.join(data_folder, \"test_data.dta\")\n",
        ")\n",
        "print(\"First few rows of the STATA file:\")\n",
        "print(df_dta.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ahQcA3hPdpps"
      },
      "source": [
        "#### Read SPSS File (.sav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bCOhCBjUdpps",
        "outputId": "ff620f54-85fd-416c-b828-c350a052be37"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the STATA file:\n",
            "    ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0  1.0  Low As  BR01  1.0   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1  2.0  Low As  BR01  2.0  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2  3.0  Low As  BR01  3.0  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3  4.0  Low As  BR06  1.0  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4  5.0  Low As  BR06  2.0  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n"
          ]
        }
      ],
      "source": [
        " # Read a .sav file\n",
        "df_sav, meta_sav = pyreadstat.read_sav(\n",
        "    os.path.join(data_folder, \"test_data.sav\")\n",
        ")\n",
        "print(\"First few rows of the STATA file:\")\n",
        "print(df_sav.head())"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "OPvS32hodppt"
      },
      "source": [
        "#### Read SAS File (.sas7bdat)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_v9TOsLOdppt",
        "outputId": "d7cd0726-914c-48a0-f68e-c22edea55da8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "First few rows of the SAS file:\n",
            "    ID   treat   var  rep     PH    TN    PN    GW  ster    DTM    SW    GAs  \\\n",
            "0  1.0  Low As  BR01  1.0   84.0  28.3  27.7  35.7  20.5  126.0  28.4  0.762   \n",
            "1  2.0  Low As  BR01  2.0  111.7  34.0  30.0  58.1  14.8  119.0  36.7  0.722   \n",
            "2  3.0  Low As  BR01  3.0  102.3  27.7  24.0  44.6   5.8  119.7  32.9  0.858   \n",
            "3  4.0  Low As  BR06  1.0  118.0  23.3  19.7  46.4  20.3  119.0  40.0  1.053   \n",
            "4  5.0  Low As  BR06  2.0  115.3  16.7  12.3  19.9  32.3  120.0  28.2  1.130   \n",
            "\n",
            "    STAs  \n",
            "0  14.60  \n",
            "1  10.77  \n",
            "2  12.69  \n",
            "3  18.23  \n",
            "4  13.72  \n"
          ]
        }
      ],
      "source": [
        "# Read a .sas7bdat file\n",
        "df_sas, meta_sas = pyreadstat.read_sas7bdat(\n",
        "    os.path.join(data_folder, \"test_data.sas7bdat\")\n",
        ")\n",
        "\n",
        "print(\"First few rows of the SAS file:\")\n",
        "print(df_sas.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "6CS-UbWTdppt"
      },
      "source": [
        "## Export Data from Python\n",
        "\n",
        "Data exporting is the process of saving data from Python to a file format that can be used by other software or systems. This is crucial for sharing results and collaborating."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O1uJ4129dppt"
      },
      "source": [
        "### Create a Sample Data Frame\n",
        "\n",
        "First, let's create a simple data frame that we will use for exporting."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PTWe-fOrdppu",
        "outputId": "e17f7b20-1749-4fdf-f678-bf7568a8b2a4"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Sample Data Frame:\n",
            "   Variety  Yield\n",
            "0      BR1    5.2\n",
            "1      BR3    6.0\n",
            "2     BR16    6.6\n",
            "3     BR17    5.6\n",
            "4     BR18    4.7\n",
            "5     BR19    5.2\n",
            "6     BR26    5.7\n",
            "7     BR27    5.9\n",
            "8     BR28    5.3\n",
            "9     BR29    6.8\n",
            "10    BR35    6.2\n",
            "11    BR36    5.8\n"
          ]
        }
      ],
      "source": [
        "# Create sample data\n",
        "rice_data = pd.DataFrame({\n",
        "    'Variety': [\"BR1\",\"BR3\", \"BR16\", \"BR17\", \"BR18\", \"BR19\",\"BR26\", \"BR27\",\"BR28\",\"BR29\",\"BR35\",\"BR36\"],\n",
        "    'Yield': [5.2,6.0,6.6,5.6,4.7,5.2,5.7, 5.9,5.3,6.8,6.2,5.8]\n",
        "})\n",
        "\n",
        "print(\"Sample Data Frame:\")\n",
        "print(rice_data)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0I7r5KuAdppu"
      },
      "source": [
        "### Write as CSV File\n",
        "\n",
        "The `to_csv()` method exports a DataFrame to a CSV file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "L3vFiYx9dppu",
        "outputId": "8ef2a396-fe42-45c1-f1d3-bd8cb0b8fddc"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data exported to CSV: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.csv\n"
          ]
        }
      ],
      "source": [
        "# Write to CSV\n",
        "output_path_csv = os.path.join(data_folder, \"rice_data.csv\") if os.path.exists(data_folder) else \"rice_data.csv\"\n",
        "\n",
        "rice_data.to_csv(\n",
        "    output_path_csv,\n",
        "    index=False  # Do not write row indices\n",
        ")\n",
        "\n",
        "print(f\" Data exported to CSV: {output_path_csv}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_LZ_QSF2dppu"
      },
      "source": [
        "### Write as Excel File\n",
        "\n",
        "The `to_excel()` method exports a DataFrame to an Excel file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "30lr6jmUdppu",
        "outputId": "23b2ffd5-6b34-445a-9fac-0906a265db4a"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data exported to Excel: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.xlsx\n"
          ]
        }
      ],
      "source": [
        "# Write to Excel\n",
        "output_path_xlsx = os.path.join(data_folder, \"rice_data.xlsx\") if os.path.exists(data_folder) else \"rice_data.xlsx\"\n",
        "\n",
        "rice_data.to_excel(\n",
        "    output_path_xlsx,\n",
        "    index=False,\n",
        "    engine='openpyxl'\n",
        ")\n",
        "\n",
        "print(f\" Data exported to Excel: {output_path_xlsx}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "k6J5zmhWdppv"
      },
      "source": [
        "### Write as JSON File\n",
        "\n",
        "The `to_json()` method exports a DataFrame to a JSON file."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4r2wzHzSdppv",
        "outputId": "5ffd9f0d-7884-4f75-eb5d-5a49ff13079e"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data exported to JSON: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.json\n"
          ]
        }
      ],
      "source": [
        "# Write to JSON\n",
        "output_path_json = os.path.join(data_folder, \"rice_data.json\") if os.path.exists(data_folder) else \"rice_data.json\"\n",
        "\n",
        "rice_data.to_json(\n",
        "    output_path_json,\n",
        "    orient='records',  # Creates a list of dictionaries\n",
        "    indent=4           # Pretty-print with indentation\n",
        ")\n",
        "\n",
        "print(f\" Data exported to JSON: {output_path_json}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vqh3Tmo0dppv"
      },
      "source": [
        "### Export to Other Statistical Software\n",
        "\n",
        "Using `pyreadstat`, we can export our DataFrame to formats used by SPSS, Stata, and SAS."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Sej_axvOdppv"
      },
      "source": [
        "#### Write STATA File (.dta)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "a3rS9P7tdpp0",
        "outputId": "63a74e9c-a268-4393-ceed-baffcbd6a5ff"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Data exported to STATA: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.dta\n"
          ]
        }
      ],
      "source": [
        "output_path_dta = os.path.join(data_folder, \"rice_data.dta\") if os.path.exists(data_folder) else \"rice_data.dta\"\n",
        "\n",
        "pyreadstat.write_dta(\n",
        "    rice_data,\n",
        "    output_path_dta\n",
        ")\n",
        "\n",
        "print(f\"Data exported to STATA: {output_path_dta}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NC8HkS1Cdpp1"
      },
      "source": [
        "#### Write SPSS File (.sav)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9qrP6HcIdpp1",
        "outputId": "b077cc2f-0fe1-4a23-86fb-b96d98c6431f"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            " Data exported to SPSS: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.sav\n"
          ]
        }
      ],
      "source": [
        "output_path_sav = os.path.join(data_folder, \"rice_data.sav\") if os.path.exists(data_folder) else \"rice_data.sav\"\n",
        "\n",
        "pyreadstat.write_sav(\n",
        "    rice_data,\n",
        "    output_path_sav\n",
        ")\n",
        "\n",
        "print(f\" Data exported to SPSS: {output_path_sav}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yQ_1A95Adpp1"
      },
      "source": [
        "### Save Python Objects (Pickle)\n",
        "\n",
        "To save Python objects (like DataFrames, lists, or dictionaries) for later use in Python, we use the `pickle` module. This is analogous to saving `.RData` or `.rds` files in R."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AyDKzpLkdpp1",
        "outputId": "ed132a99-03ef-40ec-98d0-5c21e0b920e1"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Single object saved as Pickle: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/rice_data.pkl\n",
            "Multiple objects saved as Pickle: /home/zia207/Dropbox/WebSites/Python_Website/Quarto_Projects/Python_for_Beginners/Data/multi_objects.pkl\n"
          ]
        }
      ],
      "source": [
        "import pickle\n",
        "\n",
        "# Save a single object (like R's saveRDS)\n",
        "output_path_pkl = os.path.join(data_folder, \"rice_data.pkl\") if os.path.exists(data_folder) else \"rice_data.pkl\"\n",
        "\n",
        "with open(output_path_pkl, 'wb') as file:\n",
        "    pickle.dump(rice_data, file)\n",
        "\n",
        "print(f\"Single object saved as Pickle: {output_path_pkl}\")\n",
        "\n",
        "# To load it back:\n",
        "# with open(output_path_pkl, 'rb') as file:\n",
        "#     loaded_df = pickle.load(file)\n",
        "\n",
        "# Save multiple objects (like R's save)\n",
        "multi_objects = {\n",
        "    'dataframe': rice_data,\n",
        "    'variety_list': rice_data['Variety'].tolist(),\n",
        "    'yield_list': rice_data['Yield'].tolist()\n",
        "}\n",
        "\n",
        "output_path_multi_pkl = os.path.join(data_folder, \"multi_objects.pkl\") if os.path.exists(data_folder) else \"multi_objects.pkl\"\n",
        "\n",
        "with open(output_path_multi_pkl, 'wb') as file:\n",
        "    pickle.dump(multi_objects, file)\n",
        "\n",
        "print(f\"Multiple objects saved as Pickle: {output_path_multi_pkl}\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "XubNy3r8dpp1"
      },
      "source": [
        "## Summary and Conclusion\n",
        "\n",
        "This guide covers the essential skills for importing and exporting data in Python. Key libraries like `pandas` and `pyreadstat` provide robust, easy-to-use functions for handling a wide array of file formats.\n",
        "\n",
        "- **pandas** is your go-to for CSV, Excel, and JSON.\n",
        "- **pyreadstat** is indispensable for working with SPSS, Stata, and SAS files.\n",
        "- The built-in **json** and **pickle** modules offer fine-grained control for their respective formats.\n",
        "\n",
        "By mastering these techniques, you can efficiently bring data into your Python environment for analysis and export your results for use in other applications or to share with colleagues."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "KnMUOi-bdpp2"
      },
      "source": [
        "## Resources\n",
        "\n",
        "1.  [pandas Documentation](https://pandas.pydata.org/docs/)\n",
        "2.  [pyreadstat Documentation](https://ofajardo.github.io/pyreadstat_documentation/_build/html/index.html)\n",
        "3.  [Python json Module](https://docs.python.org/3/library/json.html)\n",
        "4.  [Python pickle Module](https://docs.python.org/3/library/pickle.html)"
      ]
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.12"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}