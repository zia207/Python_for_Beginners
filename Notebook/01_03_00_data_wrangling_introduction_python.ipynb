{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/zia207/python-colab/blob/main/NoteBook/Python_for_Beginners/01-03-00-data-wrangling-introduction-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3. Introduction to Data Wrangling in Python\n",
    "\n",
    "**Data Wrangling** (also known as *data munging* or *data preprocessing*) is a crucial process in data science that involves transforming and cleaning raw data into a usable format. It includes converting data structures, handling missing or duplicated values, managing outliers, standardizing formats, and ensuring data integrity. This process is foundational — it ensures that subsequent analysis, modeling, or visualization is built on accurate, reliable data.\n",
    "\n",
    "In environmental science, finance, healthcare, and many other domains, clean, well-wrangled data enables the creation of precise maps, predictive models, and actionable insights. Consistent and accurate data wrangling is not optional — it’s essential for trustworthy decision-making.\n",
    "\n",
    "This section provides an overview of data wrangling, its importance, key steps, and the most powerful **Python packages** designed to make wrangling efficient and reproducible."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Why is Data Wrangling Important?\n",
    "\n",
    "Here are key reasons why data wrangling is indispensable:\n",
    "\n",
    "1.  **Data Quality Improvement**: Identifies and corrects errors, inconsistencies, missing values, and outliers to enhance reliability.\n",
    "2.  **Compatibility**: Harmonizes data from disparate sources (CSV, JSON, APIs, databases) into a unified structure.\n",
    "3.  **Handling Missing Values**: Applies imputation (mean, median, forward-fill), interpolation, or strategic removal to prevent bias.\n",
    "4.  **Data Transformation**: Converts data types, reshapes structures (melt/pivot), normalizes scales, and derives new features.\n",
    "5.  **Feature Engineering**: Creates meaningful predictors (features) to boost machine learning model performance.\n",
    "6.  **Outlier Detection & Handling**: Flags or adjusts extreme values using statistical or ML-based methods.\n",
    "7.  **Data Reduction**: Simplifies large datasets through sampling, aggregation, or dimensionality reduction (e.g., PCA).\n",
    "8.  **Improved Efficiency**: Clean data reduces debugging time and accelerates modeling and visualization workflows.\n",
    "9.  **Data Exploration**: Wrangling and exploration are iterative — revealing patterns, anomalies, and hypotheses.\n",
    "10. **Reproducibility**: Scripted, documented wrangling (e.g., in Jupyter Notebooks or `.py` files) ensures others can replicate your steps.\n",
    "11. **Regulatory Compliance**: Critical in healthcare, finance, and government to meet data privacy standards (e.g., GDPR, HIPAA).\n",
    "12. **Better Decision-Making**: High-quality data → accurate insights → confident, data-driven decisions.\n",
    "\n",
    "> Data wrangling is not a preliminary chore — it’s the backbone of robust analytics and AI. Data scientists often spend 60–80% of their time on this stage."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Steps of Data Wrangling\n",
    "\n",
    "The process typically follows six iterative stages [(Source: FavTutor)](https://favtutor.com/blogs/data-wrangling):\n",
    "\n",
    "1.  **Discovering**: Explore and understand the raw data — use `.head()`, `.info()`, `.describe()` to identify patterns and anomalies.\n",
    "2.  **Structuring**: Reorganize data to fit analytical needs — reshape with `melt()`/`pivot()`, set proper indices, or engineer features.\n",
    "3.  **Cleaning**:\n",
    "    - **Outliers**: Detect using boxplots, Z-scores, or IQR; handle by capping, transformation, or removal.\n",
    "    - **Missing Values**: Use `.fillna()`, interpolation, or model-based imputation (e.g., `sklearn.impute`).\n",
    "4.  **Enriching**: Enhance data through:\n",
    "    - **Feature Engineering**: Create new columns (e.g., `df['BMI'] = df['weight'] / (df['height']/100)**2`).\n",
    "    - **Merging**: Combine datasets using `pd.merge()` or `pd.concat()`.\n",
    "    - **Encoding**: Convert categories to numbers (One-Hot, Label Encoding).\n",
    "5.  **Validating**: Apply automated checks — ensure data types, value ranges, and distributions meet expectations.\n",
    "6.  **Publishing**: Export cleaned data for consumption — save as CSV, Parquet, Feather, or push to a database.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1nsgpcRuh9QhKoQ49VVlbXXXviynq9daX)\n",
    "\n",
    "source: [FavTutor](https://favtutor.com/blogs/data-wrangling)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Important Python Packages for Data Wrangling\n",
    "\n",
    "| Package         | Purpose                                                                 | Installation & Docs                                                                 |\n",
    "|-----------------|-------------------------------------------------------------------------|-------------------------------------------------------------------------------------|\n",
    "| `pandas`        | Core library for data manipulation — DataFrames, Series, I/O, cleaning   | `pip install pandas` — [pandas.pydata.org](https://pandas.pydata.org)               |\n",
    "| `numpy`         | Numerical computing — arrays, math ops, random numbers                  | `pip install numpy` — [numpy.org](https://numpy.org)                                |\n",
    "| `polars`        | Blazing-fast DataFrame library (Rust-based) for big data                | `pip install polars` — [pola.rs](https://www.pola.rs)                               |\n",
    "| `pyjanitor`     | Clean, method-chaining API for common cleaning tasks                    | `pip install pyjanitor` — [pyjanitor.readthedocs.io](https://pyjanitor.readthedocs.io) |\n",
    "| `dataprep`      | Automated EDA and cleaning (e.g., `clean_email`, `clean_country`)       | `pip install dataprep` — [dataprep.ai](https://dataprep.ai)                         |\n",
    "| `missingno`     | Visualize missing data patterns                                         | `pip install missingno` — [GitHub](https://github.com/ResidentMario/missingno)       |\n",
    "| `scikit-learn`  | Imputation, encoding, scaling, feature engineering                      | `pip install scikit-learn` — [scikit-learn.org](https://scikit-learn.org)           |\n",
    "| `dask`          | Parallel computing — scales pandas to larger-than-memory datasets       | `pip install dask` — [dask.org](https://dask.org)                                   |\n",
    "| `fuzzywuzzy`    | Fuzzy string matching for deduplication                                 | `pip install fuzzywuzzy` — [GitHub](https://github.com/seatgeek/fuzzywuzzy)         |\n",
    "| `unidecode`     | Convert Unicode text to ASCII (e.g., “José” → “Jose”)                   | `pip install unidecode` — [PyPI](https://pypi.org/project/Unidecode/)               |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Books Focused on Data Wrangling in Python\n",
    "\n",
    "1.  **Python for Data Analysis** by Wes McKinney (Creator of Pandas)\n",
    "    - [O’Reilly](https://www.oreilly.com/library/view/python-for-data/9781098104023/)\n",
    "    - Covers `pandas`, `numpy`, data cleaning, and visualization.\n",
    "    - The definitive guide — perfect for beginners and pros.\n",
    "\n",
    "2.  **Effective Data Wrangling with Python** by Tirthajyoti Sarkar\n",
    "    - Practical, hands-on examples using real-world datasets.\n",
    "    - Covers `pandas`, `numpy`, `scikit-learn`, and automation.\n",
    "\n",
    "3.  **Hands-On Data Analysis with Pandas** by Stefanie Molin\n",
    "    - Step-by-step projects — from importing to advanced wrangling.\n",
    "    - Includes time series, text data, and performance tips."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## General/Language-Agnostic Books on Data Wrangling & Cleaning\n",
    "\n",
    "4.  **Data Wrangling with Python** by Jacqueline Kazil & Katharine Jarmul\n",
    "    - Focuses on practical workflows — scraping, cleaning, storing.\n",
    "    - Great for building end-to-end pipelines.\n",
    "\n",
    "5.  **The Data Wrangling Workshop** by Brian Lipp, et al.\n",
    "    - Project-based learning — fix messy data, automate cleaning.\n",
    "    - Covers Python tools and best practices.\n",
    "\n",
    "6.  **Data Science from Scratch** by Joel Grus\n",
    "    - Builds tools from the ground up — great for understanding fundamentals.\n",
    "    - Includes chapters on cleaning and preprocessing.\n",
    "\n",
    "7.  **Feature Engineering and Selection** by Max Kuhn & Kjell Johnson\n",
    "    - Deep dive into creating and selecting predictive features.\n",
    "    - Uses R and Python examples — concepts are universal."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Quick Example: Data Wrangling with Pandas\n",
    "\n",
    "Let’s walk through a simple example using `pandas`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "name",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "age",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "salary",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "department",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "40b239d0-f563-4174-9002-793af9c4d0c6",
       "rows": [
        [
         "0",
         "Alice",
         "25.0",
         "50000.0",
         "Engineering"
        ],
        [
         "1",
         "Bob",
         null,
         "60000.0",
         "HR"
        ],
        [
         "2",
         "Charlie",
         "35.0",
         null,
         "Engineering"
        ],
        [
         "3",
         "David",
         "45.0",
         "80000.0",
         "Marketing"
        ],
        [
         "4",
         "Eve",
         "29.0",
         "55000.0",
         "HR"
        ]
       ],
       "shape": {
        "columns": 4,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>name</th>\n",
       "      <th>age</th>\n",
       "      <th>salary</th>\n",
       "      <th>department</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Alice</td>\n",
       "      <td>25.0</td>\n",
       "      <td>50000.0</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Bob</td>\n",
       "      <td>NaN</td>\n",
       "      <td>60000.0</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Charlie</td>\n",
       "      <td>35.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Engineering</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>David</td>\n",
       "      <td>45.0</td>\n",
       "      <td>80000.0</td>\n",
       "      <td>Marketing</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Eve</td>\n",
       "      <td>29.0</td>\n",
       "      <td>55000.0</td>\n",
       "      <td>HR</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      name   age   salary   department\n",
       "0    Alice  25.0  50000.0  Engineering\n",
       "1      Bob   NaN  60000.0           HR\n",
       "2  Charlie  35.0      NaN  Engineering\n",
       "3    David  45.0  80000.0    Marketing\n",
       "4      Eve  29.0  55000.0           HR"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "# Create sample messy dataset\n",
    "data = {\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'David', 'Eve'],\n",
    "    'age': [25, np.nan, 35, 45, 29],\n",
    "    'salary': [50000, 60000, np.nan, 80000, 55000],\n",
    "    'department': ['Engineering', 'HR', 'Engineering', 'Marketing', 'HR']\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "print(\"Original DataFrame:\")\n",
    "display(df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "--- Step 1: Discover ---\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5 entries, 0 to 4\n",
      "Data columns (total 4 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   name        5 non-null      object \n",
      " 1   age         4 non-null      float64\n",
      " 2   salary      4 non-null      float64\n",
      " 3   department  5 non-null      object \n",
      "dtypes: float64(2), object(2)\n",
      "memory usage: 288.0+ bytes\n",
      "None\n",
      "             age        salary\n",
      "count   4.000000      4.000000\n",
      "mean   33.500000  61250.000000\n",
      "std     8.698659  13149.778198\n",
      "min    25.000000  50000.000000\n",
      "25%    28.000000  53750.000000\n",
      "50%    32.000000  57500.000000\n",
      "75%    37.500000  65000.000000\n",
      "max    45.000000  80000.000000\n"
     ]
    }
   ],
   "source": [
    "# Step 1: Discover\n",
    "print(\"\\n--- Step 1: Discover ---\")\n",
    "print(df.info())\n",
    "print(df.describe())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2 & 3: Clean - Handle missing values\n",
    "print(\"\\n--- Step 2 & 3: Clean ---\")\n",
    "df_clean = df.copy()\n",
    "df_clean['age'].fillna(df_clean['age'].mean(), inplace=True)\n",
    "df_clean['salary'].fillna(df_clean['salary'].median(), inplace=True)\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Enrich - Feature Engineering\n",
    "print(\"\\n--- Step 4: Enrich ---\")\n",
    "df_clean['age_group'] = pd.cut(df_clean['age'], bins=[0, 30, 40, 100], labels=['Young', 'Middle', 'Senior'])\n",
    "display(df_clean)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 5: Validate\n",
    "print(\"\\n--- Step 5: Validate ---\")\n",
    "print(\"No missing values:\", df_clean.isnull().sum().sum() == 0)\n",
    "print(\"Valid age range:\", df_clean['age'].between(0, 100).all())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 6: Publish (save to CSV)\n",
    "print(\"\\n--- Step 6: Publish ---\")\n",
    "df_clean.to_csv('cleaned_data.csv', index=False)\n",
    "print(\"Data saved to 'cleaned_data.csv'\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusions\n",
    "\n",
    "Data wrangling is not just a preliminary step — it’s the backbone of robust data science. Python, with its rich ecosystem of libraries like Pandas, NumPy, Polars, and Dataprep, provides powerful, flexible, and scalable tools to tackle even the messiest datasets.\n",
    "\n",
    "Mastering data wrangling will dramatically improve your efficiency, accuracy, and confidence in any data-driven project."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "1.  **Python for Data Analysis** by Wes McKinney (Creator of Pandas)\n",
    "\n",
    "    -   [O’Reilly](https://www.oreilly.com/library/view/python-for-data/9781098104023/)\n",
    "\n",
    "    -   Covers `pandas`, `numpy`, data cleaning, and visualization.\n",
    "\n",
    "    -   The definitive guide — perfect for beginners and pros.\n",
    "\n",
    "2.  **Effective Data Wrangling with Python** by Tirthajyoti Sarkar\n",
    "\n",
    "    -   Practical, hands-on examples using real-world datasets.\n",
    "\n",
    "    -   Covers `pandas`, `numpy`, `scikit-learn`, and automation.\n",
    "\n",
    "3.  **Hands-On Data Analysis with Pandas** by Stefanie Molin\n",
    "\n",
    "    -   Step-by-step projects — from importing to advanced wrangling.\n",
    "\n",
    "    -   Includes time series, text data, and performance tips.\n",
    "\n",
    "4.  **Data Wrangling with Python** by Jacqueline Kazil & Katharine Jarmul\n",
    "\n",
    "    -   Focuses on practical workflows — scraping, cleaning, storing.\n",
    "\n",
    "    -   Great for building end-to-end pipelines.\n",
    "\n",
    "5.  **The Data Wrangling Workshop** by Brian Lipp, et al.\n",
    "\n",
    "    -   Project-based learning — fix messy data, automate cleaning.\n",
    "\n",
    "    -   Covers Python tools and best practices.\n",
    "\n",
    "6.  **Data Science from Scratch** by Joel Grus\n",
    "\n",
    "    -   Builds tools from the ground up — great for understanding fundamentals.\n",
    "\n",
    "    -   Includes chapters on cleaning and preprocessing.\n",
    "\n",
    "7.  **Feature Engineering and Selection** by Max Kuhn & Kjell Johnson\n",
    "\n",
    "    -   Deep dive into creating and selecting predictive features.\n",
    "\n",
    "    -   Uses R and Python examples — concepts are universal."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
