{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/zia207/python-colab/blob/main/NoteBook/Python_for_Beginners/01-03-01-data-wrangling-pandas-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.1 Data Wrangling with Pandas\n",
    "\n",
    "[Pandas](https://pandas.pydata.org/), is a powerful, open-source Python library used for data manipulation and analysis. It began development in 2008 at AQR Capital Management and became open-source in 2009. It has since grown with contributions from a global community and, since 2015, is a NumFOCUS-sponsored project. Key milestones include the 2012 publication of Python for Data Analysis and the first core developer sprint in 2018. Pandas offers a fast, flexible DataFrame for tasks like data alignment, missing data handling, reshaping, merging, and time series analysis, with high-performance code in Cython/C. Used across domains like finance and neuroscience, pandas aims to be the most accessible, powerful, and user-friendly open-source data analysis tool, fostering an inclusive community for all users and contributors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1cVOaeig7WVBVOYpYgwVJoxUUooDsaaOR)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Overview\n",
    "\n",
    "In this section, you will delve deeper into data manipulation using. By mastering pandas, you will be able to seamlessly transform, clean, reshape, and analyze data in a streamlined and efficient manner — much like `{dplyr}` and `{tidyr}` in R, but with even more flexibility and integration with the broader Python scientific stack.\n",
    "\n",
    ">  **Note**: This tutorial assumes you are using **Python 3.8+**  installed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "packages = ['scipy', 'numpy', 'pandas' ,'matplotlib', 'seaborn' ]\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package):\n",
    "        try:\n",
    "            import pip\n",
    "            pip.main(['install', package])\n",
    "        except ImportError:\n",
    "            print(f\"Failed to install {package}. Pip is not available.\")\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "scipy installed: True\n",
      "numpy installed: True\n",
      "pandas installed: True\n",
      "matplotlib installed: True\n",
      "seaborn installed: True\n"
     ]
    }
   ],
   "source": [
    "# Verify package availability\n",
    "for package in packages:\n",
    "    print(f\"{package} installed: {bool(importlib.util.find_spec(package))}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/lib/python3/dist-packages/scipy/__init__.py:146: UserWarning: A NumPy version >=1.17.3 and <1.25.0 is required for this version of SciPy (detected version 1.26.4\n",
      "  warnings.warn(f\"A NumPy version >={np_minversion} and <{np_maxversion}\"\n"
     ]
    }
   ],
   "source": [
    "# Optional: for better display in Jupyter\n",
    "from IPython.display import display\n",
    "pd.set_option('display.max_columns', None)\n",
    "pd.set_option('display.width', None)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load Data\n",
    "\n",
    "In this exercise we will use following CSV files:\n",
    "\n",
    "1.  `usa_division.csv`: USA division names with IDs\n",
    "\n",
    "2.  `usa_state.csv`: USA State names with ID and division ID.\n",
    "\n",
    "3.  `usa_corn_production.csv`: USA grain crop production by state from 2012-2022\n",
    "\n",
    "4.  `gp_soil_data.csv`: Soil carbon with co-variate from four states in the Greatplain region in the USA\n",
    "\n",
    "5.  `usa_geochemical_raw.csv`: Soil geochemical data for the USA, but not cleaned\n",
    "\n",
    "All data set use in this exercise can be downloaded from my [Dropbox](https://www.dropbox.com/scl/fo/fohioij7h503duitpl040/h?rlkey=3voumajiklwhgqw75fe8kby3o&dl=0) or from my [Github](https://github.com/zia207/r-colab/tree/main/Data/R_Beginners) accounts."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FIPS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COUNTY",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SOC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DEM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aspect",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Slope",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TPI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "KFactor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NDVI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SiltClay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NLCD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "FRG",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "47fdefd8-3f9c-45f3-a687-bbb53fc68da0",
       "rows": [
        [
         "0",
         "1",
         "56041",
         "56",
         "Wyoming",
         "Uinta County",
         "-111.01186",
         "41.05630002",
         "15.763",
         "2229.078613",
         "159.1877441",
         "5.671614647",
         "-0.085723579",
         "0.319999993",
         "468.3244934",
         "4.595168591",
         "0.413938969",
         "64.84269714",
         "Shrubland",
         "Fire Regime Group IV"
        ],
        [
         "1",
         "2",
         "56023",
         "56",
         "Wyoming",
         "Lincoln County",
         "-110.982973",
         "42.88349671",
         "15.883",
         "1889.400146",
         "156.8785553",
         "8.913811684",
         "4.559131622",
         "0.261212111",
         "536.3521729",
         "3.859924316",
         "0.693953156",
         "72.00454712",
         "Shrubland",
         "Fire Regime Group IV"
        ],
        [
         "2",
         "3",
         "56039",
         "56",
         "Wyoming",
         "Teton County",
         "-110.80649",
         "44.53497",
         "18.142",
         "2423.04834",
         "168.6123505",
         "4.774805069",
         "2.605886698",
         "0.216199994",
         "859.5509033",
         "0.885500014",
         "0.546603322",
         "57.18700027",
         "Forest",
         "Fire Regime Group V"
        ],
        [
         "3",
         "4",
         "56039",
         "56",
         "Wyoming",
         "Teton County",
         "-110.7344173",
         "44.43288644",
         "10.745",
         "2484.282715",
         "198.3536224",
         "7.12181139",
         "5.146931171",
         "0.181666672",
         "869.4724121",
         "0.470781118",
         "0.619101346",
         "54.99166489",
         "Forest",
         "Fire Regime Group V"
        ],
        [
         "4",
         "5",
         "56029",
         "56",
         "Wyoming",
         "Park County",
         "-110.7307901",
         "44.80635003",
         "10.479",
         "2396.19458",
         "201.3214874",
         "7.949864388",
         "3.755705833",
         "0.125510201",
         "802.9743042",
         "0.758826554",
         "0.584472179",
         "51.22857285",
         "Forest",
         "Fire Regime Group V"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>STATE_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>SOC</th>\n",
       "      <th>DEM</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>TPI</th>\n",
       "      <th>KFactor</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAT</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SiltClay</th>\n",
       "      <th>NLCD</th>\n",
       "      <th>FRG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>56041</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>-111.011860</td>\n",
       "      <td>41.056300</td>\n",
       "      <td>15.763</td>\n",
       "      <td>2229.078613</td>\n",
       "      <td>159.187744</td>\n",
       "      <td>5.671615</td>\n",
       "      <td>-0.085724</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>468.324493</td>\n",
       "      <td>4.595169</td>\n",
       "      <td>0.413939</td>\n",
       "      <td>64.842697</td>\n",
       "      <td>Shrubland</td>\n",
       "      <td>Fire Regime Group IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56023</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Lincoln County</td>\n",
       "      <td>-110.982973</td>\n",
       "      <td>42.883497</td>\n",
       "      <td>15.883</td>\n",
       "      <td>1889.400146</td>\n",
       "      <td>156.878555</td>\n",
       "      <td>8.913812</td>\n",
       "      <td>4.559132</td>\n",
       "      <td>0.261212</td>\n",
       "      <td>536.352173</td>\n",
       "      <td>3.859924</td>\n",
       "      <td>0.693953</td>\n",
       "      <td>72.004547</td>\n",
       "      <td>Shrubland</td>\n",
       "      <td>Fire Regime Group IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>56039</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>-110.806490</td>\n",
       "      <td>44.534970</td>\n",
       "      <td>18.142</td>\n",
       "      <td>2423.048340</td>\n",
       "      <td>168.612350</td>\n",
       "      <td>4.774805</td>\n",
       "      <td>2.605887</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>859.550903</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.546603</td>\n",
       "      <td>57.187000</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>56039</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>-110.734417</td>\n",
       "      <td>44.432886</td>\n",
       "      <td>10.745</td>\n",
       "      <td>2484.282715</td>\n",
       "      <td>198.353622</td>\n",
       "      <td>7.121811</td>\n",
       "      <td>5.146931</td>\n",
       "      <td>0.181667</td>\n",
       "      <td>869.472412</td>\n",
       "      <td>0.470781</td>\n",
       "      <td>0.619101</td>\n",
       "      <td>54.991665</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>56029</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Park County</td>\n",
       "      <td>-110.730790</td>\n",
       "      <td>44.806350</td>\n",
       "      <td>10.479</td>\n",
       "      <td>2396.194580</td>\n",
       "      <td>201.321487</td>\n",
       "      <td>7.949864</td>\n",
       "      <td>3.755706</td>\n",
       "      <td>0.125510</td>\n",
       "      <td>802.974304</td>\n",
       "      <td>0.758827</td>\n",
       "      <td>0.584472</td>\n",
       "      <td>51.228573</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   FIPS  STATE_ID    STATE          COUNTY   Longitude   Latitude  \\\n",
       "0   1  56041        56  Wyoming    Uinta County -111.011860  41.056300   \n",
       "1   2  56023        56  Wyoming  Lincoln County -110.982973  42.883497   \n",
       "2   3  56039        56  Wyoming    Teton County -110.806490  44.534970   \n",
       "3   4  56039        56  Wyoming    Teton County -110.734417  44.432886   \n",
       "4   5  56029        56  Wyoming     Park County -110.730790  44.806350   \n",
       "\n",
       "      SOC          DEM      Aspect     Slope       TPI   KFactor         MAP  \\\n",
       "0  15.763  2229.078613  159.187744  5.671615 -0.085724  0.320000  468.324493   \n",
       "1  15.883  1889.400146  156.878555  8.913812  4.559132  0.261212  536.352173   \n",
       "2  18.142  2423.048340  168.612350  4.774805  2.605887  0.216200  859.550903   \n",
       "3  10.745  2484.282715  198.353622  7.121811  5.146931  0.181667  869.472412   \n",
       "4  10.479  2396.194580  201.321487  7.949864  3.755706  0.125510  802.974304   \n",
       "\n",
       "        MAT      NDVI   SiltClay       NLCD                   FRG  \n",
       "0  4.595169  0.413939  64.842697  Shrubland  Fire Regime Group IV  \n",
       "1  3.859924  0.693953  72.004547  Shrubland  Fire Regime Group IV  \n",
       "2  0.885500  0.546603  57.187000     Forest   Fire Regime Group V  \n",
       "3  0.470781  0.619101  54.991665     Forest   Fire Regime Group V  \n",
       "4  0.758827  0.584472  51.228573     Forest   Fire Regime Group V  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "div = pd.read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/usa_division.csv\")\n",
    "state = pd.read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/usa_state.csv\")\n",
    "corn = pd.read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/usa_corn_production.csv\")\n",
    "soil = pd.read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/gp_soil_data.csv\")\n",
    "geo_raw = pd.read_csv(\"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/usa_geochemical_raw.csv\")\n",
    "\n",
    "# Display first few rows of soil data\n",
    "display(soil.head())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "FIPS",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE_ID",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "STATE",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "COUNTY",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "Longitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Latitude",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SOC",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "DEM",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Aspect",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "Slope",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "TPI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "KFactor",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAP",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "MAT",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NDVI",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "SiltClay",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "NLCD",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "FRG",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "e11aa9b6-50ce-4e2e-a2b3-e9bfa14ecd5c",
       "rows": [
        [
         "0",
         "1",
         "56041",
         "56",
         "Wyoming",
         "Uinta County",
         "-111.01186",
         "41.05630002",
         "15.763",
         "2229.078613",
         "159.1877441",
         "5.671614647",
         "-0.085723579",
         "0.319999993",
         "468.3244934",
         "4.595168591",
         "0.413938969",
         "64.84269714",
         "Shrubland",
         "Fire Regime Group IV"
        ],
        [
         "1",
         "2",
         "56023",
         "56",
         "Wyoming",
         "Lincoln County",
         "-110.982973",
         "42.88349671",
         "15.883",
         "1889.400146",
         "156.8785553",
         "8.913811684",
         "4.559131622",
         "0.261212111",
         "536.3521729",
         "3.859924316",
         "0.693953156",
         "72.00454712",
         "Shrubland",
         "Fire Regime Group IV"
        ],
        [
         "2",
         "3",
         "56039",
         "56",
         "Wyoming",
         "Teton County",
         "-110.80649",
         "44.53497",
         "18.142",
         "2423.04834",
         "168.6123505",
         "4.774805069",
         "2.605886698",
         "0.216199994",
         "859.5509033",
         "0.885500014",
         "0.546603322",
         "57.18700027",
         "Forest",
         "Fire Regime Group V"
        ],
        [
         "3",
         "4",
         "56039",
         "56",
         "Wyoming",
         "Teton County",
         "-110.7344173",
         "44.43288644",
         "10.745",
         "2484.282715",
         "198.3536224",
         "7.12181139",
         "5.146931171",
         "0.181666672",
         "869.4724121",
         "0.470781118",
         "0.619101346",
         "54.99166489",
         "Forest",
         "Fire Regime Group V"
        ],
        [
         "4",
         "5",
         "56029",
         "56",
         "Wyoming",
         "Park County",
         "-110.7307901",
         "44.80635003",
         "10.479",
         "2396.19458",
         "201.3214874",
         "7.949864388",
         "3.755705833",
         "0.125510201",
         "802.9743042",
         "0.758826554",
         "0.584472179",
         "51.22857285",
         "Forest",
         "Fire Regime Group V"
        ]
       ],
       "shape": {
        "columns": 19,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ID</th>\n",
       "      <th>FIPS</th>\n",
       "      <th>STATE_ID</th>\n",
       "      <th>STATE</th>\n",
       "      <th>COUNTY</th>\n",
       "      <th>Longitude</th>\n",
       "      <th>Latitude</th>\n",
       "      <th>SOC</th>\n",
       "      <th>DEM</th>\n",
       "      <th>Aspect</th>\n",
       "      <th>Slope</th>\n",
       "      <th>TPI</th>\n",
       "      <th>KFactor</th>\n",
       "      <th>MAP</th>\n",
       "      <th>MAT</th>\n",
       "      <th>NDVI</th>\n",
       "      <th>SiltClay</th>\n",
       "      <th>NLCD</th>\n",
       "      <th>FRG</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>56041</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Uinta County</td>\n",
       "      <td>-111.011860</td>\n",
       "      <td>41.056300</td>\n",
       "      <td>15.763</td>\n",
       "      <td>2229.078613</td>\n",
       "      <td>159.187744</td>\n",
       "      <td>5.671615</td>\n",
       "      <td>-0.085724</td>\n",
       "      <td>0.320000</td>\n",
       "      <td>468.324493</td>\n",
       "      <td>4.595169</td>\n",
       "      <td>0.413939</td>\n",
       "      <td>64.842697</td>\n",
       "      <td>Shrubland</td>\n",
       "      <td>Fire Regime Group IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>56023</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Lincoln County</td>\n",
       "      <td>-110.982973</td>\n",
       "      <td>42.883497</td>\n",
       "      <td>15.883</td>\n",
       "      <td>1889.400146</td>\n",
       "      <td>156.878555</td>\n",
       "      <td>8.913812</td>\n",
       "      <td>4.559132</td>\n",
       "      <td>0.261212</td>\n",
       "      <td>536.352173</td>\n",
       "      <td>3.859924</td>\n",
       "      <td>0.693953</td>\n",
       "      <td>72.004547</td>\n",
       "      <td>Shrubland</td>\n",
       "      <td>Fire Regime Group IV</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>56039</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>-110.806490</td>\n",
       "      <td>44.534970</td>\n",
       "      <td>18.142</td>\n",
       "      <td>2423.048340</td>\n",
       "      <td>168.612350</td>\n",
       "      <td>4.774805</td>\n",
       "      <td>2.605887</td>\n",
       "      <td>0.216200</td>\n",
       "      <td>859.550903</td>\n",
       "      <td>0.885500</td>\n",
       "      <td>0.546603</td>\n",
       "      <td>57.187000</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>56039</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Teton County</td>\n",
       "      <td>-110.734417</td>\n",
       "      <td>44.432886</td>\n",
       "      <td>10.745</td>\n",
       "      <td>2484.282715</td>\n",
       "      <td>198.353622</td>\n",
       "      <td>7.121811</td>\n",
       "      <td>5.146931</td>\n",
       "      <td>0.181667</td>\n",
       "      <td>869.472412</td>\n",
       "      <td>0.470781</td>\n",
       "      <td>0.619101</td>\n",
       "      <td>54.991665</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>56029</td>\n",
       "      <td>56</td>\n",
       "      <td>Wyoming</td>\n",
       "      <td>Park County</td>\n",
       "      <td>-110.730790</td>\n",
       "      <td>44.806350</td>\n",
       "      <td>10.479</td>\n",
       "      <td>2396.194580</td>\n",
       "      <td>201.321487</td>\n",
       "      <td>7.949864</td>\n",
       "      <td>3.755706</td>\n",
       "      <td>0.125510</td>\n",
       "      <td>802.974304</td>\n",
       "      <td>0.758827</td>\n",
       "      <td>0.584472</td>\n",
       "      <td>51.228573</td>\n",
       "      <td>Forest</td>\n",
       "      <td>Fire Regime Group V</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ID   FIPS  STATE_ID    STATE          COUNTY   Longitude   Latitude  \\\n",
       "0   1  56041        56  Wyoming    Uinta County -111.011860  41.056300   \n",
       "1   2  56023        56  Wyoming  Lincoln County -110.982973  42.883497   \n",
       "2   3  56039        56  Wyoming    Teton County -110.806490  44.534970   \n",
       "3   4  56039        56  Wyoming    Teton County -110.734417  44.432886   \n",
       "4   5  56029        56  Wyoming     Park County -110.730790  44.806350   \n",
       "\n",
       "      SOC          DEM      Aspect     Slope       TPI   KFactor         MAP  \\\n",
       "0  15.763  2229.078613  159.187744  5.671615 -0.085724  0.320000  468.324493   \n",
       "1  15.883  1889.400146  156.878555  8.913812  4.559132  0.261212  536.352173   \n",
       "2  18.142  2423.048340  168.612350  4.774805  2.605887  0.216200  859.550903   \n",
       "3  10.745  2484.282715  198.353622  7.121811  5.146931  0.181667  869.472412   \n",
       "4  10.479  2396.194580  201.321487  7.949864  3.755706  0.125510  802.974304   \n",
       "\n",
       "        MAT      NDVI   SiltClay       NLCD                   FRG  \n",
       "0  4.595169  0.413939  64.842697  Shrubland  Fire Regime Group IV  \n",
       "1  3.859924  0.693953  72.004547  Shrubland  Fire Regime Group IV  \n",
       "2  0.885500  0.546603  57.187000     Forest   Fire Regime Group V  \n",
       "3  0.470781  0.619101  54.991665     Forest   Fire Regime Group V  \n",
       "4  0.758827  0.584472  51.228573     Forest   Fire Regime Group V  "
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# Load datasets\n",
    "div = pd.read_csv(\"https://github.com/zia207/python-colab/raw/refs/heads/main/Data/Python_for_Beginners/Data/usa_division.csv\")\n",
    "state = pd.read_csv(\"https://github.com/zia207/python-colab/raw/refs/heads/main/Data/Python_for_Beginners/Data/usa_state.csv\")\n",
    "corn = pd.read_csv(\"https://github.com/zia207/python-colab/raw/refs/heads/main/Data/Python_for_Beginners/Data/usa_corn_production.csv\")\n",
    "soil = pd.read_csv(\"https://github.com/zia207/python-colab/raw/refs/heads/main/Data/Python_for_Beginners/Data/gp_soil_data.csv\")\n",
    "geo_raw = pd.read_csv(\"https://github.com/zia207/python-colab/raw/refs/heads/main/Data/Python_for_Beginners/Data/usa_geochemical_raw.csv\")\n",
    "\n",
    "# Display first few rows of soil data\n",
    "display(soil.head())\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Method Chaining & Pipe Equivalent in Python\n",
    "\n",
    "Python doesn’t have a native pipe operator like R’s `%>%`, but **pandas supports method chaining**, and you can use `.pipe()` for custom functions or complex pipelines."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Example of method chaining\n",
    "result = (\n",
    "    corn\n",
    "    .merge(state, on='STATE_ID', how='inner')\n",
    "    .merge(div, on='DIVISION_ID', how='inner')\n",
    "    .query('MT > MT.mean()')\n",
    "    .groupby('DIVISION_NAME')\n",
    "    .agg(avg_prod=('MT', 'mean'))\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Joining DataFrames (like `inner_join`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 465 entries, 0 to 464\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   STATE_ID       465 non-null    int64  \n",
      " 1   YEAR           465 non-null    int64  \n",
      " 2   MT             465 non-null    float64\n",
      " 3   STATE_NAME     465 non-null    object \n",
      " 4   DIVISION_ID    465 non-null    int64  \n",
      " 5   DIVISION_NAME  465 non-null    object \n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 21.9+ KB\n"
     ]
    }
   ],
   "source": [
    "# Join corn with state, then with division\n",
    "df_corn = (\n",
    "    corn\n",
    "    .merge(state, on='STATE_ID', how='inner')  # equivalent to inner_join\n",
    "    .merge(div, on='DIVISION_ID', how='inner')\n",
    ")\n",
    "\n",
    "# Display structure\n",
    "df_corn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Relocate Columns (like `relocate`)\n",
    "\n",
    "Pandas doesn’t have `relocate()`, but you can reorder columns easily:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define desired column order\n",
    "cols_order = [\n",
    "    'DIVISION_ID', 'DIVISION_NAME', 'STATE_ID', 'STATE_NAME',\n",
    "    'YEAR', 'MT'\n",
    "] + [col for col in df_corn.columns if col not in [\n",
    "    'DIVISION_ID', 'DIVISION_NAME', 'STATE_ID', 'STATE_NAME', 'YEAR', 'MT'\n",
    "]]\n",
    "\n",
    "df_corn = df_corn[cols_order]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Rename Columns (like `rename`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_corn = df_corn.rename(columns={'STATE_ID': 'STATE_FIPS'})\n",
    "print(df_corn.columns.tolist())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Combine Join + Relocate + Rename in One Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 465 entries, 0 to 464\n",
      "Data columns (total 6 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   DIVISION_ID    465 non-null    int64  \n",
      " 1   DIVISION_NAME  465 non-null    object \n",
      " 2   STATE_FIPS     465 non-null    int64  \n",
      " 3   STATE_NAME     465 non-null    object \n",
      " 4   YEAR           465 non-null    int64  \n",
      " 5   MT             465 non-null    float64\n",
      "dtypes: float64(1), int64(3), object(2)\n",
      "memory usage: 21.9+ KB\n"
     ]
    }
   ],
   "source": [
    "df_corn = (\n",
    "    corn\n",
    "    .merge(state, on='STATE_ID', how='inner')\n",
    "    .merge(div, on='DIVISION_ID', how='inner')\n",
    "    .rename(columns={'STATE_ID': 'STATE_FIPS'})\n",
    "    # Reorder columns\n",
    "    .pipe(lambda df: df[[\n",
    "        'DIVISION_ID', 'DIVISION_NAME', 'STATE_FIPS', 'STATE_NAME',\n",
    "        'YEAR', 'MT'\n",
    "    ] + [c for c in df.columns if c not in [\n",
    "        'DIVISION_ID', 'DIVISION_NAME', 'STATE_FIPS', 'STATE_NAME', 'YEAR', 'MT'\n",
    "    ]]])\n",
    ")\n",
    "\n",
    "# Optional: Use .info() or .head() instead of glimpse\n",
    "df_corn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Select Columns (like `select`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 465 entries, 0 to 464\n",
      "Data columns (total 3 columns):\n",
      " #   Column      Non-Null Count  Dtype  \n",
      "---  ------      --------------  -----  \n",
      " 0   STATE_NAME  465 non-null    object \n",
      " 1   YEAR        465 non-null    int64  \n",
      " 2   MT          465 non-null    float64\n",
      "dtypes: float64(1), int64(1), object(1)\n",
      "memory usage: 11.0+ KB\n"
     ]
    }
   ],
   "source": [
    "df_state = df_corn[['STATE_NAME', 'YEAR', 'MT']].copy()\n",
    "df_state.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Filter Rows (like `filter`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Filter by one condition\n",
    "df_01 = df_corn[df_corn['DIVISION_NAME'] == 'East North Central']\n",
    "\n",
    "# Filter by multiple conditions with .isin()\n",
    "df_02 = df_corn[df_corn['DIVISION_NAME'].isin([\n",
    "    'East North Central', 'East South Central', 'Middle Atlantic'\n",
    "])]\n",
    "\n",
    "# Filter with OR (|)\n",
    "df_03 = df_corn[\n",
    "    (df_corn['DIVISION_NAME'] == 'East North Central') |\n",
    "    (df_corn['DIVISION_NAME'] == 'Middle Atlantic')\n",
    "]\n",
    "\n",
    "# Filter NY in Middle Atlantic\n",
    "df_ny = df_corn[\n",
    "    (df_corn['DIVISION_NAME'] == 'Middle Atlantic') &\n",
    "    (df_corn['STATE_NAME'] == 'New York')\n",
    "]\n",
    "\n",
    "# Filter where MT > global mean\n",
    "global_mean = df_corn['MT'].mean()\n",
    "mean_prod = df_corn[df_corn['MT'] > global_mean]\n",
    "\n",
    "# Filter for 2017 AND above mean\n",
    "mean_prod_2017 = df_corn[\n",
    "    (df_corn['MT'] > global_mean) &\n",
    "    (df_corn['YEAR'] == 2017)\n",
    "]\n",
    "\n",
    "# Filter states starting with 'A' (using str.startswith)\n",
    "state_a = df_corn[df_corn['STATE_NAME'].str.startswith('A')]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Summarize Data (like `summarise`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean: 8360401.602795699\n",
      "Median: 2072749.4\n",
      "mean      8.360402e+06\n",
      "median    2.072749e+06\n",
      "std       1.440890e+07\n",
      "min       2.691000e+02\n",
      "max       6.961238e+07\n",
      "Name: MT, dtype: float64\n"
     ]
    }
   ],
   "source": [
    "# Single summary\n",
    "print(\"Mean:\", df_corn['MT'].mean())\n",
    "print(\"Median:\", df_corn['MT'].median())\n",
    "\n",
    "# Multiple summaries in one go\n",
    "summary_stats = df_corn['MT'].agg(['mean', 'median', 'std', 'min', 'max'])\n",
    "print(summary_stats)\n",
    "\n",
    "# Grouped summaries\n",
    "division_summary = (\n",
    "    df_corn\n",
    "    .groupby('DIVISION_NAME', as_index=False)\n",
    "    .agg(Prod_MT=('MT', 'mean'))\n",
    ")\n",
    "\n",
    "# For 2020 only\n",
    "division_2020_summary = (\n",
    "    df_corn[df_corn['YEAR'] == 2020]\n",
    "    .groupby('DIVISION_NAME', as_index=False)\n",
    "    .agg(Prod_MT=('MT', 'mean'))\n",
    "    .assign(Prod_1000_MT = lambda x: x['Prod_MT'] / 1000)\n",
    ")\n",
    "\n",
    "# Summarize multiple numeric columns by group\n",
    "soil_summary = (\n",
    "    soil\n",
    "    .groupby('STATE', as_index=False)\n",
    "    [['SOC', 'NDVI', 'MAP', 'MAT']]\n",
    "    .mean()\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Create New Columns (like `mutate`)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 465 entries, 0 to 464\n",
      "Data columns (total 7 columns):\n",
      " #   Column         Non-Null Count  Dtype  \n",
      "---  ------         --------------  -----  \n",
      " 0   DIVISION_ID    465 non-null    int64  \n",
      " 1   DIVISION_NAME  465 non-null    object \n",
      " 2   STATE_FIPS     465 non-null    int64  \n",
      " 3   STATE_NAME     465 non-null    object \n",
      " 4   YEAR           465 non-null    int64  \n",
      " 5   MT             465 non-null    float64\n",
      " 6   MT_1000        465 non-null    float64\n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 25.6+ KB\n"
     ]
    }
   ],
   "source": [
    "df_corn = df_corn.assign(MT_1000 = df_corn['MT'] / 10000)\n",
    "df_corn.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Group By + Summarize + Mutate Chain"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        DIVISION_NAME       Prod_MT  Prod_1000_MT\n",
      "0  East North Central  2.274695e+07  22746.951840\n",
      "1  East South Central  3.350119e+06   3350.119375\n",
      "2     Middle Atlantic  1.929554e+06   1929.553633\n",
      "3            Mountain  6.512214e+05    651.221443\n",
      "4             Pacific  3.917310e+05    391.731033\n",
      "5      South Atlantic  1.223075e+06   1223.074588\n",
      "6  West North Central  2.828109e+07  28281.091243\n",
      "7  West South Central  3.009964e+06   3009.963650\n"
     ]
    }
   ],
   "source": [
    "result = (\n",
    "    df_corn\n",
    "    .query('YEAR == 2020')\n",
    "    .groupby('DIVISION_NAME', as_index=False)\n",
    "    .agg(Prod_MT=('MT', 'mean'))\n",
    "    .assign(Prod_1000_MT = lambda x: x['Prod_MT'] / 1000)\n",
    ")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Pivoting Data (like `pivot_wider`, `pivot_longer`)\n",
    "\n",
    "Pivoting a DataFrame is a data manipulation technique that involves reorganizing the structure of the data to create a new view. This technique is particularly useful when working with large datasets because it can help to make the data more manageable and easier to analyze. In R, pivoting a DataFrame typically involves using the dplyr or tidyr packages to transform the data from a long format to a wide format or vice versa. This allows for easier analysis of the data and can help to highlight patterns, trends, and relationships that might otherwise be difficult to see. Overall, pivoting a DataFrame is an important tool in the data analyst's toolkit and can be used to gain valuable insights into complex datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "YEAR  STATE_FIPS  STATE_NAME       2012       2013       2014       2015  \\\n",
      "0              1     Alabama   734352.8  1101529.2  1151061.8   914829.3   \n",
      "1              4     Arizona   158504.4   229297.9   149359.9   192034.1   \n",
      "2              5    Arkansas  3142399.9  4110445.0  2517526.9  2045951.0   \n",
      "3              6  California   823003.5   873298.1   398166.0   239280.6   \n",
      "4              8    Colorado  3412162.2  3261024.2  3745681.8  3426640.9   \n",
      "\n",
      "YEAR       2016       2017       2018       2019       2020       2021  \\\n",
      "0      960170.7   996875.6   970839.3  1138869.1  1284291.8  1407742.3   \n",
      "1      273064.4   158504.4   111765.9   217105.3   148801.1    82757.6   \n",
      "2     3236003.9  2765825.0  2965479.6  3267247.5  2827677.3  3879292.8   \n",
      "3      469924.8   339361.9   285638.1   256045.5   285003.1   238772.6   \n",
      "4     4071581.0  4722109.3  3929587.5  4061674.5  3123348.9  3768289.0   \n",
      "\n",
      "YEAR       2022  \n",
      "0      869233.9  \n",
      "1      223531.8  \n",
      "2     3054130.3  \n",
      "3       89920.8  \n",
      "4     3012091.0  \n"
     ]
    }
   ],
   "source": [
    "# First filter out states with incomplete years\n",
    "complete_states = (\n",
    "    df_corn\n",
    "    .groupby('STATE_NAME')\n",
    "    .filter(lambda x: len(x) >= 11)  # keep if 11+ years\n",
    ")\n",
    "\n",
    "corn_wider = (\n",
    "    complete_states\n",
    "    [['STATE_FIPS', 'STATE_NAME', 'YEAR', 'MT']]\n",
    "    .pivot(index=['STATE_FIPS', 'STATE_NAME'], columns='YEAR', values='MT')\n",
    "    .reset_index()\n",
    ")\n",
    "print(corn_wider.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# First filter out states with incomplete years\n",
    "complete_states = (\n",
    "    df_corn\n",
    "    .groupby('STATE_NAME')\n",
    "    .filter(lambda x: len(x) >= 11)  # keep if 11+ years\n",
    ")\n",
    "\n",
    "corn_wider = (\n",
    "    complete_states\n",
    "    [['STATE_FIPS', 'STATE_NAME', 'YEAR', 'MT']]\n",
    "    .pivot(index=['STATE_FIPS', 'STATE_NAME'], columns='YEAR', values='MT')\n",
    "    .reset_index()\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "     STATE_FIPS     STATE_NAME  YEAR          MT\n",
      "0             1        Alabama  2012    734352.8\n",
      "1             4        Arizona  2012    158504.4\n",
      "2             5       Arkansas  2012   3142399.9\n",
      "3             6     California  2012    823003.5\n",
      "4             8       Colorado  2012   3412162.2\n",
      "..          ...            ...   ...         ...\n",
      "446          51       Virginia  2022   1442288.2\n",
      "447          53     Washington  2022    419122.1\n",
      "448          54  West Virginia  2022    149359.9\n",
      "449          55      Wisconsin  2022  13853891.5\n",
      "450          56        Wyoming  2022    217638.7\n",
      "\n",
      "[451 rows x 4 columns]\n"
     ]
    }
   ],
   "source": [
    "# pivot_longer equivalent\n",
    "year_cols = [str(year) for year in range(2012, 2023)]\n",
    "\n",
    "corn_wider.columns = corn_wider.columns.astype(str)\n",
    "\n",
    "corn_longer = (\n",
    "    corn_wider\n",
    "    .melt(\n",
    "        id_vars=['STATE_FIPS', 'STATE_NAME'],\n",
    "        value_vars=year_cols,\n",
    "        var_name='YEAR',\n",
    "        value_name='MT'\n",
    "    )\n",
    "    .astype({'YEAR': int})  # Convert YEAR back to int\n",
    ")\n",
    "print(corn_longer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  variable stat       value\n",
      "0      SOC  min    0.408000\n",
      "1      DEM  min  258.648804\n",
      "2     NDVI  min    0.142433\n",
      "3      MAP  min  193.913223\n",
      "4      MAT  min   -0.591064\n"
     ]
    }
   ],
   "source": [
    "# Summary Stats + Pivot Longer (Advanced)\n",
    "summary_stats = (\n",
    "    soil[['SOC', 'DEM', 'NDVI', 'MAP', 'MAT']]\n",
    "    .agg(['min', lambda x: x.quantile(0.25), 'median', lambda x: x.quantile(0.75), 'max', 'mean', 'std'])\n",
    "    .T\n",
    "    .reset_index()\n",
    "    .melt(id_vars='index', var_name='stat', value_name='value')\n",
    "    .assign(variable = lambda x: x['index'])\n",
    "    [['variable', 'stat', 'value']]\n",
    ")\n",
    "print(summary_stats.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Additional Powerful pandas Functions for Data Wrangling"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "|  TASK | Pandas Function  |\n",
    "|----|----|\n",
    "| Handle missing values | `.fillna()`,`.dropna()`,`.interpolate()`,`.bfill()`,`.ffill()` |\n",
    "| String manipulation | `.str.replace()`,`.str.contains()`,`.str.extract()`,`.str.split()` |\n",
    "| Datetime handling | `pd.to_datetime()`,`.dt.year`,`.dt.month`, etc. |\n",
    "| Binning / Categorizing | `pd.cut()`,`pd.qcut()`,`.astype('category')` |\n",
    "| Duplicates | `.duplicated()`,`.drop_duplicates()` |\n",
    "| Value counts | `.value_counts()`,`.nunique()` |\n",
    "| Cross-tabulation | `pd.crosstab()` |\n",
    "| Reshaping | `.stack()`,`.unstack()`,`.explode()`(for lists) |\n",
    "| Conditional assignment | `np.where()`,`pd.Series.mask()`,`pd.Series.where()`,`pd.cut()` |\n",
    "| Advanced groupby | `.groupby().apply()`,`.transform()`,`.filter()` |\n",
    "| Memory optimization | `.astype()`,`pd.read_csv(..., dtype=...)`,`pd.Categorical` |\n",
    "| Sampling | `.sample(n=100)`,`.sample(frac=0.1)` |\n",
    "| Rolling windows | `.rolling()`,`.expanding()`,`.ewm()` |"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Missing Data Handling\n",
    "Beyond `.fillna()` and `.dropna()`, pandas offers nuanced control:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   col category\n",
      "0  1.0        A\n",
      "1  2.0        A\n",
      "2  NaN        A\n",
      "3  4.0        B\n",
      "4  5.0        B\n",
      "5  NaN        B\n",
      "6  7.0        A\n",
      "7  8.0        A\n"
     ]
    }
   ],
   "source": [
    "df = pd.DataFrame({\n",
    "    'col': [1, 2, np.nan, 4, 5, np.nan, 7, 8],\n",
    "    'category': ['A', 'A', 'A', 'B', 'B', 'B', 'A', 'A']\n",
    "})\n",
    "print(df)\n",
    "\n",
    "\n",
    "# Forward fill (last observation carried forward)\n",
    "df['col'].ffill()\n",
    "\n",
    "# Backward fill\n",
    "df['col'].bfill()\n",
    "\n",
    "# Interpolate missing values (linear, time, spline, etc.)\n",
    "df['col'].interpolate(method='linear')\n",
    "\n",
    "# Fill conditionally\n",
    "df['col'] = df['col'].mask(df['col'] < 0, np.nan)  # Set negatives to NaN\n",
    "df['col'] = df['col'].where(df['col'] > 0, 0)      # Set negatives to 0\n",
    "\n",
    "# Fill with grouped mean (transform keeps original index)\n",
    "df['col_filled'] = df.groupby('category')['col'].transform(lambda x: x.fillna(x.mean()))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Efficient String Operations with `.str` Accessor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "text",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "1f6ecf69-e4ba-49f3-a3fc-5f476fde628b",
       "rows": [
        [
         "0",
         "Leading And Trailing Spaces"
        ],
        [
         "1",
         "Multiple   Internal   Spaces"
        ],
        [
         "2",
         "No Spaces"
        ]
       ],
       "shape": {
        "columns": 1,
        "rows": 3
       }
      },
      "text/plain": [
       "0     Leading And Trailing Spaces\n",
       "1    Multiple   Internal   Spaces\n",
       "2                       No Spaces\n",
       "Name: text, dtype: object"
      ]
     },
     "execution_count": 37,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = {'full_name': ['Alice Smith', 'Bob Johnson', 'Charlie Brown'],\n",
    "    'email': ['alice@example.com', 'bob.johnson@domain.net', 'charlie'],\n",
    "    'text': ['  Leading and trailing spaces  ', 'Multiple   internal   spaces', 'No spaces'],\n",
    "    'price_str': ['$10.50', '20', '30.75 USD']}\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "# Extract numbers from string\n",
    "df['price_str'].str.extract(r'(\\d+\\.?\\d*)').astype(float)\n",
    "\n",
    "# Split into multiple columns\n",
    "df[['first', 'last']] = df['full_name'].str.split(' ', n=1, expand=True)\n",
    "\n",
    "# Check patterns\n",
    "df['email'].str.contains('@', na=False)  # Handle NaN safely\n",
    "\n",
    "# Replace with regex\n",
    "df['text'].str.replace(r'\\s+', ' ', regex=True)  # Normalize whitespace\n",
    "\n",
    "# Case conversion, stripping, etc.\n",
    "df['text'].str.lower().str.strip().str.title()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Datetime & Time-Series Power Tools"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original DataFrame (first 5 rows):\n",
      "     date_str  sales  visitors\n",
      "0  2023-01-01    202       240\n",
      "1  2023-01-02    535       451\n",
      "2  2023-01-03    960       267\n",
      "3  2023-01-04    370        93\n",
      "4  2023-01-05    206       211\n",
      "\n",
      "============================================================\n",
      "\n",
      " After converting 'date_str' to datetime 'date':\n",
      "     date_str       date\n",
      "0  2023-01-01 2023-01-01\n",
      "1  2023-01-02 2023-01-02\n",
      "2  2023-01-03 2023-01-03\n",
      "\n",
      "============================================================\n",
      "\n",
      " After extracting datetime components:\n",
      "        date  year month_name  dayofweek\n",
      "0 2023-01-01  2023    January          6\n",
      "1 2023-01-02  2023    January          0\n",
      "2 2023-01-03  2023    January          1\n",
      "3 2023-01-04  2023    January          2\n",
      "4 2023-01-05  2023    January          3\n",
      "\n",
      "============================================================\n",
      "\n",
      " Monthly Resample (sum of sales & visitors):\n",
      "            sales  visitors\n",
      "date                       \n",
      "2023-01-31  14891      8990\n",
      "2023-02-28  17081      7113\n",
      "2023-03-31  15903      8523\n",
      "\n",
      "============================================================\n",
      "\n",
      " 7-Day Rolling Average of Sales (last 5 rows):\n",
      "            sales  sales_7d_rolling\n",
      "date                               \n",
      "2023-03-27    665        507.714286\n",
      "2023-03-28    205        511.285714\n",
      "2023-03-29    871        541.285714\n",
      "2023-03-30    921        534.142857\n",
      "2023-03-31    576        546.857143\n",
      "\n",
      "============================================================\n",
      "\n",
      " After timezone conversion (first 3 rows):\n",
      "        date                  date_utc              date_eastern\n",
      "0 2023-01-01 2023-01-01 00:00:00+00:00 2022-12-31 19:00:00-05:00\n",
      "1 2023-01-02 2023-01-02 00:00:00+00:00 2023-01-01 19:00:00-05:00\n",
      "2 2023-01-03 2023-01-03 00:00:00+00:00 2023-01-02 19:00:00-05:00\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/1426820335.py:30: FutureWarning: 'M' is deprecated and will be removed in a future version, please use 'ME' instead.\n",
      "  monthly_summary = df.set_index('date').resample('M').sum(numeric_only=True)\n"
     ]
    }
   ],
   "source": [
    "#  CREATE SAMPLE DATAFRAME\n",
    "np.random.seed(42)  # For reproducible results\n",
    "date_range = pd.date_range(start='2023-01-01', end='2023-03-31', freq='D')\n",
    "df = pd.DataFrame({\n",
    "    'date_str': date_range.strftime('%Y-%m-%d'),  # Simulate string dates\n",
    "    'sales': np.random.randint(100, 1000, size=len(date_range)),\n",
    "    'visitors': np.random.randint(50, 500, size=len(date_range))\n",
    "})\n",
    "\n",
    "print(\" Original DataFrame (first 5 rows):\")\n",
    "print(df.head())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  1. Convert to datetime (auto-infer format)\n",
    "df['date'] = pd.to_datetime(df['date_str'])\n",
    "print(\" After converting 'date_str' to datetime 'date':\")\n",
    "print(df[['date_str', 'date']].head(3))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  2. Extract components\n",
    "df['year'] = df['date'].dt.year\n",
    "df['month_name'] = df['date'].dt.month_name()\n",
    "df['dayofweek'] = df['date'].dt.dayofweek  # Monday=0, Sunday=6\n",
    "\n",
    "print(\" After extracting datetime components:\")\n",
    "print(df[['date', 'year', 'month_name', 'dayofweek']].head(5))\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  3. Resample time series (daily to monthly)\n",
    "monthly_summary = df.set_index('date').resample('M').sum(numeric_only=True)\n",
    "print(\" Monthly Resample (sum of sales & visitors):\")\n",
    "print(monthly_summary[['sales', 'visitors']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  4. Rolling windows with time-based offsets (7-day rolling average)\n",
    "# We'll create a new DataFrame to avoid SettingWithCopyWarning\n",
    "df_rolling = df.set_index('date').copy()\n",
    "df_rolling['sales_7d_rolling'] = df_rolling['sales'].rolling('7D').mean()\n",
    "\n",
    "print(\" 7-Day Rolling Average of Sales (last 5 rows):\")\n",
    "print(df_rolling[['sales', 'sales_7d_rolling']].tail())\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  5. Timezone handling\n",
    "# Localize to UTC, then convert to US/Eastern\n",
    "df['date_utc'] = df['date'].dt.tz_localize('UTC')\n",
    "df['date_eastern'] = df['date_utc'].dt.tz_convert('US/Eastern')\n",
    "\n",
    "print(\" After timezone conversion (first 3 rows):\")\n",
    "print(df[['date', 'date_utc', 'date_eastern']].head(3))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Advanced Reshaping & Nesting\n",
    "\n",
    "`xplode()`— Unnest list-like columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "int64",
         "type": "integer"
        },
        {
         "name": "team",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "members",
         "rawType": "object",
         "type": "string"
        }
       ],
       "ref": "54bc77b4-0d47-4ca7-a137-dcbf275ab816",
       "rows": [
        [
         "0",
         "A",
         "Alice"
        ],
        [
         "0",
         "A",
         "Bob"
        ],
        [
         "1",
         "B",
         "Charlie"
        ],
        [
         "1",
         "B",
         "Dana"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 4
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>team</th>\n",
       "      <th>members</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Alice</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>A</td>\n",
       "      <td>Bob</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Charlie</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>B</td>\n",
       "      <td>Dana</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  team  members\n",
       "0    A    Alice\n",
       "0    A      Bob\n",
       "1    B  Charlie\n",
       "1    B     Dana"
      ]
     },
     "execution_count": 47,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# If a column contains lists\n",
    "df = pd.DataFrame({\n",
    "    'team': ['A', 'B'],\n",
    "    'members': [['Alice', 'Bob'], ['Charlie', 'Dana']]\n",
    "})\n",
    "df.explode('members')  # Each list element becomes a row"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`melt()` + `pivot_table()` — Flexible reshaping"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Pivoted Back to Wide Format (Mean Scores):\n",
      "subject    id  English  History  Math  Science\n",
      "0        S001     84.5     87.5  90.5     92.5\n",
      "1        S002     90.5     82.5  94.5     87.5\n",
      "2        S003     80.5     87.5  77.5     82.5\n",
      "3        S004     87.5     92.5  87.5     97.5\n",
      "4        S005     94.5     89.5  92.5     90.5\n"
     ]
    }
   ],
   "source": [
    "# CREATE SAMPLE DATASET (WIDE FORMAT)\n",
    "# Each row = one student, each subject column = their score\n",
    "df = pd.DataFrame({\n",
    "    'id': ['S001', 'S002', 'S003', 'S004', 'S005'],\n",
    "    'Math': [88, 92, 75, 85, 90],\n",
    "    'Science': [90, 85, 80, 95, 88],\n",
    "    'English': [82, 88, 78, 85, 92],\n",
    "    'History': [85, 80, 85, 90, 87]\n",
    "})\n",
    "\n",
    "# . MELT: Go from Wide to Long\n",
    "# 'id' stays as identifier, other columns become 'category' and 'value'\n",
    "df_melted = df.melt(\n",
    "    id_vars=['id'],           # Keep these columns as-is\n",
    "    var_name='subject',       # Name for the former column headers\n",
    "    value_name='score'        # Name for the values\n",
    ")\n",
    "\n",
    "\n",
    "#  PIVOT_TABLE: Go back to Wide (with aggregation)\n",
    "# Simulate duplicate entries by appending a modified copy\n",
    "df_melted_duplicate = df_melted.copy()\n",
    "df_melted_duplicate['score'] += 5  # Add 5 points to all scores\n",
    "df_long_with_duplicates = pd.concat([df_melted, df_melted_duplicate], ignore_index=True)\n",
    "\n",
    "\n",
    "# Pivot back to wide format, aggregating duplicates with mean\n",
    "df_pivoted = df_long_with_duplicates.pivot_table(\n",
    "    index='id',               # Rows\n",
    "    columns='subject',        # Columns\n",
    "    values='score',           # Values to aggregate\n",
    "    aggfunc='mean',           # How to aggregate (handles duplicates!)\n",
    "    fill_value=0              # Replace NaN with 0 if any\n",
    ").reset_index()               # Move 'id' from index back to column\n",
    "\n",
    "print(\" Pivoted Back to Wide Format (Mean Scores):\")\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Conditional Logic & Binning\n",
    "`np.select()` — Multi-condition assignment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "subject    id  English  History  Math  Science grade\n",
      "0        S001     84.5     87.5  90.5     92.5     A\n",
      "1        S002     90.5     82.5  94.5     87.5     A\n",
      "2        S003     80.5     87.5  77.5     82.5     C\n",
      "3        S004     87.5     92.5  87.5     97.5     B\n",
      "4        S005     94.5     89.5  92.5     90.5     A\n"
     ]
    }
   ],
   "source": [
    "conditions = [\n",
    "    df_pivoted['Math'] >= 90,\n",
    "    df_pivoted['Math'] >= 80,\n",
    "    df_pivoted['Math'] >= 70\n",
    "]\n",
    "choices = ['A', 'B', 'C']\n",
    "df_pivoted['grade'] = np.select(conditions, choices, default='F')\n",
    "print(df_pivoted)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "`pd.cut()` / `pd.qcut()` — Binning continuous variables"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧩 After applying pd.cut() for age groups:\n",
      "         name  age age_group\n",
      "0    Person_1   56    Middle\n",
      "1    Person_2   19     Young\n",
      "2    Person_3   76    Senior\n",
      "3    Person_4   65    Senior\n",
      "4    Person_5   25     Young\n",
      "5    Person_6   87    Senior\n",
      "6    Person_7   91    Senior\n",
      "7    Person_8   79    Senior\n",
      "8    Person_9   79    Senior\n",
      "9   Person_10   92    Senior\n",
      "10  Person_11   28     Young\n",
      "11  Person_12    7     Child\n",
      "12  Person_13   26     Young\n",
      "13  Person_14   57    Middle\n",
      "14  Person_15    6     Child\n",
      "15  Person_16   92    Senior\n",
      "16  Person_17   34     Young\n",
      "17  Person_18   42    Middle\n",
      "18  Person_19    6     Child\n",
      "19  Person_20   68    Senior\n",
      "\n",
      "============================================================\n",
      "\n",
      "After applying pd.qcut() for income quartiles:\n",
      "         name  income income_quantile\n",
      "0    Person_1  123355  Q2 (Lower-Mid)\n",
      "1    Person_2  105305  Q2 (Lower-Mid)\n",
      "2    Person_3  179765    Q4 (Highest)\n",
      "3    Person_4  150608    Q4 (Highest)\n",
      "4    Person_5  176730    Q4 (Highest)\n",
      "5    Person_6  104478  Q2 (Lower-Mid)\n",
      "6    Person_7  142537  Q3 (Upper-Mid)\n",
      "7    Person_8  169503    Q4 (Highest)\n",
      "8    Person_9  150523  Q3 (Upper-Mid)\n",
      "9   Person_10   22747     Q1 (Lowest)\n",
      "10  Person_11  143855  Q3 (Upper-Mid)\n",
      "11  Person_12   85725     Q1 (Lowest)\n",
      "12  Person_13  149981  Q3 (Upper-Mid)\n",
      "13  Person_14  104654  Q2 (Lower-Mid)\n",
      "14  Person_15  186845    Q4 (Highest)\n",
      "15  Person_16  139346  Q3 (Upper-Mid)\n",
      "16  Person_17   87435  Q2 (Lower-Mid)\n",
      "17  Person_18   76886     Q1 (Lowest)\n",
      "18  Person_19   86803     Q1 (Lowest)\n",
      "19  Person_20   51551     Q1 (Lowest)\n",
      "\n",
      "============================================================\n",
      "\n",
      " Age Group Distribution:\n",
      "age_group\n",
      "Child     3\n",
      "Young     5\n",
      "Middle    3\n",
      "Senior    9\n",
      "Name: count, dtype: int64\n",
      "\n",
      " Income Quartile Distribution:\n",
      "income_quantile\n",
      "Q1 (Lowest)       5\n",
      "Q2 (Lower-Mid)    5\n",
      "Q3 (Upper-Mid)    5\n",
      "Q4 (Highest)      5\n",
      "Name: count, dtype: int64\n"
     ]
    }
   ],
   "source": [
    "#  CREATE SAMPLE DATAFRAME\n",
    "np.random.seed(42)  # For reproducible results\n",
    "df = pd.DataFrame({\n",
    "    'name': [f'Person_{i}' for i in range(1, 21)],\n",
    "    'age': np.random.randint(5, 95, size=20),      # Ages between 5 and 94\n",
    "    'income': np.random.randint(20000, 200000, size=20)  # Income between 20K and 200K\n",
    "})\n",
    "\n",
    "\n",
    "# . Custom Bins: Categorize age into groups\n",
    "df['age_group'] = pd.cut(\n",
    "    df['age'],\n",
    "    bins=[0, 18, 35, 60, 100],\n",
    "    labels=['Child', 'Young', 'Middle', 'Senior'],\n",
    "    include_lowest=True  # Ensures 0 is included\n",
    ")\n",
    "\n",
    "print(\" After applying pd.cut() for age groups:\")\n",
    "print(df[['name', 'age', 'age_group']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "# . Quantile-Based Bins: Divide income into quartiles\n",
    "df['income_quantile'] = pd.qcut(\n",
    "    df['income'],\n",
    "    q=4,\n",
    "    labels=['Q1 (Lowest)', 'Q2 (Lower-Mid)', 'Q3 (Upper-Mid)', 'Q4 (Highest)'],\n",
    "    duplicates='drop'  # Safeguard in case of duplicate quantile edges\n",
    ")\n",
    "\n",
    "print(\"After applying pd.qcut() for income quartiles:\")\n",
    "print(df[['name', 'income', 'income_quantile']])\n",
    "print(\"\\n\" + \"=\"*60 + \"\\n\")\n",
    "\n",
    "#  Optional: Show distribution\n",
    "print(\" Age Group Distribution:\")\n",
    "print(df['age_group'].value_counts().sort_index())\n",
    "print(\"\\n Income Quartile Distribution:\")\n",
    "print(df['income_quantile'].value_counts().sort_index())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization\n",
    "\n",
    "When working with large datasets, performance can become a bottleneck. Here are some tips to optimize your pandas code:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Original DataFrame Info:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 100000 entries, 0 to 99999\n",
      "Data columns (total 7 columns):\n",
      " #   Column            Non-Null Count   Dtype  \n",
      "---  ------            --------------   -----  \n",
      " 0   transaction_id    100000 non-null  int64  \n",
      " 1   product_category  100000 non-null  object \n",
      " 2   region            100000 non-null  object \n",
      " 3   price             100000 non-null  float64\n",
      " 4   quantity          100000 non-null  int64  \n",
      " 5   discount_pct      100000 non-null  float64\n",
      " 6   is_high_value     100000 non-null  int64  \n",
      "dtypes: float64(2), int64(3), object(2)\n",
      "memory usage: 5.3+ MB\n",
      "None\n",
      "\n",
      "Memory usage: 16.02 MB\n",
      "\n",
      "======================================================================\n",
      "\n",
      " After using .eval() to create 3 new columns:\n",
      "    price  quantity  discount_pct  total_revenue  net_revenue\n",
      "0   43.91         4          0.15         175.64      149.294\n",
      "1  296.99         5          0.00        1484.95     1484.950\n",
      "2  401.45         4          0.10        1605.80     1445.220\n",
      "\n",
      "======================================================================\n",
      "\n",
      " After .query(): 4,566 rows match criteria\n",
      "    product_category region   price  discount_pct\n",
      "29       Electronics   East  478.66          0.05\n",
      "31       Electronics   East  376.24          0.10\n",
      "108      Electronics   West  468.04          0.05\n",
      "123      Electronics   East  413.98          0.10\n",
      "127            Books   West  446.57          0.05\n",
      "\n",
      "======================================================================\n",
      "\n",
      "  Unique values before conversion:\n",
      "  product_category: 5 unique values → ['Clothing', 'Sports', 'Home & Kitchen', 'Electronics', 'Books']\n",
      "  region: 4 unique values → ['South', 'West', 'East', 'North']\n",
      "\n",
      " After converting to 'category' dtype:\n",
      "transaction_id         int64\n",
      "product_category    category\n",
      "region              category\n",
      "price                float64\n",
      "quantity               int64\n",
      "discount_pct         float64\n",
      "is_high_value       category\n",
      "total_revenue        float64\n",
      "discount_amount      float64\n",
      "net_revenue          float64\n",
      "dtype: object\n",
      "\n",
      "New memory usage: 5.63 MB\n",
      "Memory saved: 5.63 MB vs original\n",
      "\n",
      " Memory usage per column (after optimization):\n",
      "  Index: 0.1 KB\n",
      "  transaction_id: 781.2 KB\n",
      "  product_category: 98.1 KB\n",
      "  region: 98.1 KB\n",
      "  price: 781.2 KB\n",
      "  quantity: 781.2 KB\n",
      "  discount_pct: 781.2 KB\n",
      "  is_high_value: 97.8 KB\n",
      "  total_revenue: 781.2 KB\n",
      "  discount_amount: 781.2 KB\n",
      "  net_revenue: 781.2 KB\n",
      "\n",
      "======================================================================\n",
      "\n",
      " REALISTIC WORKFLOW: Filter + Compute + GroupBy\n",
      "Final Aggregated Result:\n",
      "   product_category region  avg_net_revenue  total_transactions  \\\n",
      "0          Clothing   East          1503.05                1774   \n",
      "1          Clothing  North          1518.98                1744   \n",
      "2          Clothing  South          1497.96                1758   \n",
      "3          Clothing   West          1505.61                1708   \n",
      "4       Electronics   East          1494.20                2036   \n",
      "5       Electronics  North          1495.07                2037   \n",
      "6       Electronics  South          1512.00                2130   \n",
      "7       Electronics   West          1508.47                2120   \n",
      "8    Home & Kitchen   East          1492.48                1344   \n",
      "9    Home & Kitchen  North          1507.21                1378   \n",
      "10   Home & Kitchen  South          1499.23                1370   \n",
      "11   Home & Kitchen   West          1522.43                1435   \n",
      "12           Sports   East          1507.01                 618   \n",
      "13           Sports  North          1518.62                 679   \n",
      "14           Sports  South          1514.29                 622   \n",
      "15           Sports   West          1512.98                 692   \n",
      "\n",
      "    avg_profit_margin  \n",
      "0                0.96  \n",
      "1                0.97  \n",
      "2                0.96  \n",
      "3                0.96  \n",
      "4                0.96  \n",
      "5                0.96  \n",
      "6                0.97  \n",
      "7                0.97  \n",
      "8                0.96  \n",
      "9                0.96  \n",
      "10               0.96  \n",
      "11               0.97  \n",
      "12               0.96  \n",
      "13               0.97  \n",
      "14               0.96  \n",
      "15               0.97  \n"
     ]
    }
   ],
   "source": [
    "# CREATE SAMPLE DATAFRAME — Simulate 100K Sales Records\n",
    "np.random.seed(42)\n",
    "n = 100_000\n",
    "\n",
    "df = pd.DataFrame({\n",
    "    'transaction_id': range(1, n + 1),\n",
    "    'product_category': np.random.choice(\n",
    "        ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Sports'], \n",
    "        size=n, \n",
    "        p=[0.3, 0.25, 0.2, 0.15, 0.1]  # Uneven probabilities\n",
    "    ),\n",
    "    'region': np.random.choice(\n",
    "        ['North', 'South', 'East', 'West'], \n",
    "        size=n\n",
    "    ),\n",
    "    'price': np.round(np.random.uniform(10.0, 500.0, size=n), 2),\n",
    "    'quantity': np.random.randint(1, 6, size=n),\n",
    "    'discount_pct': np.random.choice([0.0, 0.05, 0.1, 0.15, 0.2], size=n, p=[0.5, 0.2, 0.15, 0.1, 0.05])\n",
    "})\n",
    "\n",
    "# Create a flag column\n",
    "df['is_high_value'] = (df['price'] > 300).astype(int)\n",
    "\n",
    "print(\" Original DataFrame Info:\")\n",
    "print(df.info())\n",
    "print(f\"\\nMemory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 1. Use eval() for complex expressions (avoids intermediate copies)\n",
    "# Calculate total revenue and net revenue in one efficient step\n",
    "df = df.eval('''\n",
    "    total_revenue = price * quantity\n",
    "    discount_amount = total_revenue * discount_pct\n",
    "    net_revenue = total_revenue - discount_amount\n",
    "''')\n",
    "\n",
    "print(\" After using .eval() to create 3 new columns:\")\n",
    "print(df[['price', 'quantity', 'discount_pct', 'total_revenue', 'net_revenue']].head(3))\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# 🔍 2. Use query() for filtering (cleaner syntax + often faster)\n",
    "# Filter: High-value items in Electronics or Books, with discount, in East or West\n",
    "df_filtered = df.query(\n",
    "    \"is_high_value == 1 and \"\n",
    "    \"product_category in ['Electronics', 'Books'] and \"\n",
    "    \"discount_pct > 0 and \"\n",
    "    \"region in ['East', 'West']\"\n",
    ")\n",
    "\n",
    "print(f\" After .query(): {len(df_filtered):,} rows match criteria\")\n",
    "print(df_filtered[['product_category', 'region', 'price', 'discount_pct']].head())\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "#  3. Convert to category dtype for low-cardinality string columns\n",
    "# Check unique values first\n",
    "print(\"  Unique values before conversion:\")\n",
    "for col in ['product_category', 'region']:\n",
    "    print(f\"  {col}: {df[col].nunique()} unique values → {list(df[col].unique())}\")\n",
    "\n",
    "# Convert to category\n",
    "df['product_category'] = df['product_category'].astype('category')\n",
    "df['region'] = df['region'].astype('category')\n",
    "df['is_high_value'] = df['is_high_value'].astype('category')  # Also good for binary flags\n",
    "\n",
    "print(\"\\n After converting to 'category' dtype:\")\n",
    "print(df.dtypes)\n",
    "print(f\"\\nNew memory usage: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB\")\n",
    "print(f\"Memory saved: {df.memory_usage(deep=True).sum() / 1024**2:.2f} MB vs original\")\n",
    "\n",
    "# Show memory usage per column\n",
    "print(\"\\n Memory usage per column (after optimization):\")\n",
    "mem_usage = df.memory_usage(deep=True) / 1024\n",
    "for col, mem_kb in mem_usage.items():\n",
    "    print(f\"  {col}: {mem_kb:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "#  Combine all optimizations in a realistic workflow\n",
    "print(\" REALISTIC WORKFLOW: Filter + Compute + GroupBy\")\n",
    "\n",
    "result = (\n",
    "    df\n",
    "    .query(\"net_revenue > 1000 and product_category != 'Books'\")\n",
    "    .eval('profit_margin = (net_revenue - 50) / net_revenue')  # Assume $50 fixed cost\n",
    "    .groupby(['product_category', 'region'], observed=True)  # observed=True for categories\n",
    "    .agg(\n",
    "        avg_net_revenue=('net_revenue', 'mean'),\n",
    "        total_transactions=('transaction_id', 'count'),\n",
    "        avg_profit_margin=('profit_margin', 'mean')\n",
    "    )\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\"Final Aggregated Result:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Method Chaining Enhancements with `.pipe()`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Final Result after Pipeline:\n",
      "  region  log_income  income_per_capita\n",
      "0   East       10.41           14296.69\n",
      "1  North       10.44           14335.13\n",
      "2  South       10.40           13410.24\n",
      "3   West       10.42           12879.74\n",
      "\n",
      "Shape after removing outliers: (920, 4)\n",
      "\n",
      " Outlier Removal Summary:\n",
      "Original records: 1,000\n",
      "Records after removal: 920\n",
      "Outliers removed: 80 (8.0%)\n",
      "\n",
      " Income IQR Bounds:\n",
      "Q1: $22,142.19\n",
      "Q3: $64,728.53\n",
      "IQR: $42,586.35\n",
      "Lower bound: $-41,737.33\n",
      "Upper bound: $128,608.05\n",
      "Outliers are values < $-41,737.33 or > $128,608.05\n"
     ]
    }
   ],
   "source": [
    "#  SAMPLE DATAFRAME\n",
    "np.random.seed(42)  # For reproducible results\n",
    "n = 1000\n",
    "\n",
    "# Generate base data\n",
    "df = pd.DataFrame({\n",
    "    'household_id': range(1, n + 1),\n",
    "    'region': np.random.choice(\n",
    "        ['North', 'South', 'East', 'West'], \n",
    "        size=n,\n",
    "        p=[0.25, 0.25, 0.25, 0.25]\n",
    "    ),\n",
    "    'household_size': np.random.choice(\n",
    "        [1, 2, 3, 4, 5], \n",
    "        size=n,\n",
    "        p=[0.15, 0.35, 0.30, 0.15, 0.05]\n",
    "    ),\n",
    "    'income': np.random.lognormal(mean=10.5, sigma=0.8, size=n).round(2)\n",
    "})\n",
    "\n",
    "# Intentionally add some extreme outliers\n",
    "outlier_indices = np.random.choice(df.index, size=30, replace=False)\n",
    "df.loc[outlier_indices, 'income'] = df['income'].max() * np.random.uniform(2, 5, size=30)\n",
    "\n",
    "# Define outlier removal function\n",
    "def remove_outliers(df, col):\n",
    "    q1 = df[col].quantile(0.25)\n",
    "    q3 = df[col].quantile(0.75)\n",
    "    iqr = q3 - q1\n",
    "    lower_bound = q1 - 1.5 * iqr\n",
    "    upper_bound = q3 + 1.5 * iqr\n",
    "    return df[(df[col] >= lower_bound) & (df[col] <= upper_bound)]\n",
    "\n",
    "# Define feature engineering function\n",
    "def add_features(df):\n",
    "    return df.assign(\n",
    "        log_income = np.log1p(df['income']),           # log(1 + income) to handle zeros\n",
    "        income_per_capita = df['income'] / df['household_size']\n",
    "    )\n",
    "\n",
    "# Chain everything together\n",
    "result = (\n",
    "    df\n",
    "    .pipe(remove_outliers, 'income')\n",
    "    .pipe(add_features)\n",
    "    .groupby('region')\n",
    "    .agg({\n",
    "        'log_income': 'mean', \n",
    "        'income_per_capita': 'median'\n",
    "    })\n",
    "    .round(2)\n",
    "    .reset_index()\n",
    ")\n",
    "\n",
    "print(\" Final Result after Pipeline:\")\n",
    "print(result)\n",
    "print(f\"\\nShape after removing outliers: {df.pipe(remove_outliers, 'income').shape}\")\n",
    "\n",
    "# Show outlier removal statistics\n",
    "original_count = len(df)\n",
    "cleaned_count = len(df.pipe(remove_outliers, 'income'))\n",
    "removed_count = original_count - cleaned_count\n",
    "\n",
    "print(f\"\\n Outlier Removal Summary:\")\n",
    "print(f\"Original records: {original_count:,}\")\n",
    "print(f\"Records after removal: {cleaned_count:,}\")\n",
    "print(f\"Outliers removed: {removed_count:,} ({removed_count/original_count:.1%})\")\n",
    "\n",
    "#  Show IQR bounds for income\n",
    "q1 = df['income'].quantile(0.25)\n",
    "q3 = df['income'].quantile(0.75)\n",
    "iqr = q3 - q1\n",
    "lower_bound = q1 - 1.5 * iqr\n",
    "upper_bound = q3 + 1.5 * iqr\n",
    "\n",
    "print(f\"\\n Income IQR Bounds:\")\n",
    "print(f\"Q1: ${q1:,.2f}\")\n",
    "print(f\"Q3: ${q3:,.2f}\")\n",
    "print(f\"IQR: ${iqr:,.2f}\")\n",
    "print(f\"Lower bound: ${lower_bound:,.2f}\")\n",
    "print(f\"Upper bound: ${upper_bound:,.2f}\")\n",
    "print(f\"Outliers are values < ${lower_bound:,.2f} or > ${upper_bound:,.2f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Validation & Quality Checks"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Check for duplicates:\n",
      "Total duplicate rows: 50\n",
      "\n",
      "Sample of duplicate rows (first 5):\n",
      "    id        category  price\n",
      "0  103     Electronics  38.61\n",
      "1  436           Books  25.86\n",
      "2  271  Home & Kitchen  36.85\n",
      "3  107  Home & Kitchen  29.31\n",
      "4   72           Books  58.35\n",
      "5  701        Clothing  71.19\n",
      "6   21     Electronics  17.66\n",
      "7  615        Clothing  11.42\n",
      "8  122  Home & Kitchen   7.62\n",
      "9  467  Home & Kitchen  49.72\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Drop duplicates (keeping last occurrence):\n",
      "Shape after dropping duplicates: (556, 6)\n",
      "Rows removed: 494\n",
      "\n",
      "Sample of cleaned data (first 5 rows):\n",
      "     id        category  price  quantity\n",
      "56   14        Clothing  37.30         1\n",
      "58  777        Clothing  20.27         4\n",
      "65  428  Home & Kitchen  85.84         4\n",
      "66  509           Books  70.78         2\n",
      "69  206        Clothing  69.93         3\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Value counts with percentages:\n",
      "Category Distribution (%):\n",
      "  Electronics: 30.4%\n",
      "  Clothing: 23.6%\n",
      "  Home & Kitchen: 23.2%\n",
      "  Books: 14.4%\n",
      "  Sports: 8.5%\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Check data types and memory usage:\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 556 entries, 56 to 1049\n",
      "Data columns (total 6 columns):\n",
      " #   Column     Non-Null Count  Dtype         \n",
      "---  ------     --------------  -----         \n",
      " 0   id         556 non-null    int64         \n",
      " 1   category   556 non-null    object        \n",
      " 2   price      556 non-null    float64       \n",
      " 3   quantity   556 non-null    int64         \n",
      " 4   region     556 non-null    object        \n",
      " 5   sale_date  556 non-null    datetime64[ns]\n",
      "dtypes: datetime64[ns](1), float64(1), int64(2), object(2)\n",
      "memory usage: 91.3 KB\n",
      "\n",
      "Total memory usage: 91.3 KB\n",
      "\n",
      "======================================================================\n",
      "\n",
      "Describe with percentiles [0.01, 0.25, 0.5, 0.75, 0.99]:\n",
      "           id   price  quantity\n",
      "count  556.00  556.00    556.00\n",
      "mean   394.46   46.51      3.00\n",
      "std    235.32   43.40      1.39\n",
      "min      1.00    2.96      1.00\n",
      "1%       8.55    4.87      1.00\n",
      "25%    183.75   19.82      2.00\n",
      "50%    393.50   34.00      3.00\n",
      "75%    607.25   57.96      4.00\n",
      "99%    792.45  191.76      5.00\n",
      "max    798.00  426.02      5.00\n",
      "\n",
      "Price column detailed statistics:\n",
      "count    556.00\n",
      "mean      46.51\n",
      "std       43.40\n",
      "min        2.96\n",
      "1%         4.87\n",
      "25%       19.82\n",
      "50%       34.00\n",
      "75%       57.96\n",
      "99%      191.76\n",
      "max      426.02\n",
      "Name: price, dtype: float64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/tmp/ipykernel_10088/640330235.py:16: FutureWarning: 'H' is deprecated and will be removed in a future version, please use 'h' instead.\n",
      "  'sale_date': pd.date_range(start='2023-01-01', periods=n, freq='H')\n"
     ]
    }
   ],
   "source": [
    "# Simulate Product Sales with Duplicates\n",
    "np.random.seed(42)\n",
    "n = 1000\n",
    "\n",
    "# Generate base data\n",
    "df = pd.DataFrame({\n",
    "    'id': np.random.randint(1, 800, size=n),  # Only 800 unique IDs for 1000 rows → guarantees duplicates\n",
    "    'category': np.random.choice(\n",
    "        ['Electronics', 'Clothing', 'Home & Kitchen', 'Books', 'Sports'], \n",
    "        size=n,\n",
    "        p=[0.3, 0.25, 0.2, 0.15, 0.1]  # Uneven probabilities\n",
    "    ),\n",
    "    'price': np.round(np.random.lognormal(mean=3.5, sigma=0.8, size=n), 2),\n",
    "    'quantity': np.random.randint(1, 6, size=n),\n",
    "    'region': np.random.choice(['North', 'South', 'East', 'West'], size=n),\n",
    "    'sale_date': pd.date_range(start='2023-01-01', periods=n, freq='H')\n",
    "})\n",
    "\n",
    "# Intentionally create duplicates by appending first 50 rows\n",
    "df_with_duplicates = pd.concat([df, df.head(50)], ignore_index=True)\n",
    "\n",
    "\n",
    "# Check for duplicates\n",
    "print(\"Check for duplicates:\")\n",
    "duplicate_count = df_with_duplicates.duplicated().sum()\n",
    "print(f\"Total duplicate rows: {duplicate_count}\")\n",
    "\n",
    "# Show what duplicates look like\n",
    "print(\"\\nSample of duplicate rows (first 5):\")\n",
    "duplicates = df_with_duplicates[df_with_duplicates.duplicated(keep=False)]\n",
    "print(duplicates.head(10)[['id', 'category', 'price']])  # Show first 10 including originals\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Drop duplicates (keep last occurrence)\n",
    "print(\"Drop duplicates (keeping last occurrence):\")\n",
    "df_clean = df_with_duplicates.drop_duplicates(subset=['id'], keep='last')\n",
    "print(f\"Shape after dropping duplicates: {df_clean.shape}\")\n",
    "print(f\"Rows removed: {len(df_with_duplicates) - len(df_clean)}\")\n",
    "\n",
    "# Show sample of cleaned data\n",
    "print(\"\\nSample of cleaned data (first 5 rows):\")\n",
    "print(df_clean.head()[['id', 'category', 'price', 'quantity']])\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Value counts with percentages\n",
    "print(\"Value counts with percentages:\")\n",
    "category_percentages = (\n",
    "    df_clean['category']\n",
    "    .value_counts(normalize=True)\n",
    "    .mul(100)\n",
    "    .round(1)\n",
    "    .sort_values(ascending=False)\n",
    ")\n",
    "\n",
    "print(\"Category Distribution (%):\")\n",
    "for category, percentage in category_percentages.items():\n",
    "    print(f\"  {category}: {percentage}%\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Check data types and memory usage\n",
    "print(\"Check data types and memory usage:\")\n",
    "df_clean.info(memory_usage='deep')\n",
    "print(f\"\\nTotal memory usage: {df_clean.memory_usage(deep=True).sum() / 1024:.1f} KB\")\n",
    "\n",
    "print(\"\\n\" + \"=\"*70 + \"\\n\")\n",
    "\n",
    "# Describe with custom percentiles\n",
    "print(\"Describe with percentiles [0.01, 0.25, 0.5, 0.75, 0.99]:\")\n",
    "numeric_cols = df_clean.select_dtypes(include=[np.number]).columns.tolist()\n",
    "description = df_clean[numeric_cols].describe(percentiles=[0.01, 0.25, 0.5, 0.75, 0.99])\n",
    "\n",
    "# Round for cleaner display\n",
    "description = description.round(2)\n",
    "print(description)\n",
    "\n",
    "# Optional: Show only price column for focus\n",
    "print(\"\\nPrice column detailed statistics:\")\n",
    "print(df_clean['price'].describe(percentiles=[0.01, 0.25, 0.5, 0.75, 0.99]).round(2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning Messy Data (Geochemical Example)\n",
    "\n",
    "This exercise is designed to provide you with a more in-depth understanding of how to clean and prepare data for analysis. Data cleaning involves a set of processes that help to ensure that your data is accurate, consistent, and complete. In this exercise, you will be introduced to various techniques that can be used to address common issues in data such as missing values, data below detection limits, and spatial characters.\n",
    "\n",
    "Missing values can occur when data is not collected or recorded for a particular variable. It is important to address missing values, as they can lead to inaccurate results and bias in your analysis. You will learn how to identify missing values and how to handle them using techniques such as imputation and deletion.\n",
    "\n",
    "Data below detection limits, also known as censored data, are values that are below the limit of detection for a particular measurement. These values can be problematic as they can skew your analysis and lead to inaccurate results. You will learn how to identify censored data and how to handle it using techniques such as substitution and regression.\n",
    "\n",
    "Spatial characters are characters that are used to represent geographical locations in data, such as postcodes or zip codes. However, these characters can sometimes be recorded incorrectly or inconsistently, which can lead to issues in your analysis. You will learn how to identify and clean spatial characters to ensure that your data is accurate and consistent.\n",
    "\n",
    "By the end of this exercise, you will have a solid understanding of how to clean and prepare your data for analysis, which will help you to obtain more accurate and reliable results.\n",
    "\n",
    "We will use National Geochemical Database from United States Geological Survey (USGS) as a part of the USGS Geochemical Landscapes Project (Smith et al., 2011)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "             N  Missing   Min     Max\n",
      "Element                              \n",
      "Arsenic   4809        0  0.30  1110.0\n",
      "Cadmium   4809        0  0.05    46.6\n",
      "Lead      4809        0  0.25  2200.0\n",
      "Chromium  4809        0  0.50  3850.0\n",
      "Rows after dropping all missing: 1178\n",
      "Missing after imputation:\n",
      "Arsenic     0\n",
      "Cadmium     0\n",
      "Lead        0\n",
      "Chromium    0\n",
      "dtype: int64\n"
     ]
    }
   ],
   "source": [
    "# Load and select/rename columns\n",
    "mf_geo = (\n",
    "    geo_raw\n",
    "    [['A_LabID', 'StateID', 'LandCover1', 'A_Depth', 'A_C_Tot', 'A_C_Inorg', 'A_C_Org', 'A_As', 'A_Cd', 'A_Pb', 'A_Cr']]\n",
    "    .rename(columns={\n",
    "        'A_LabID': 'LAB_ID',\n",
    "        'LandCover1': 'LandCover',\n",
    "        'A_Depth': 'Soil_depth',\n",
    "        'A_C_Tot': 'Total_Carbon',\n",
    "        'A_C_Inorg': 'Inorg_Carbon',\n",
    "        'A_C_Org': 'Organic_Carbon',\n",
    "        'A_As': 'Arsenic',\n",
    "        'A_Cd': 'Cadmium',\n",
    "        'A_Pb': 'Lead',\n",
    "        'A_Cr': 'Chromium'\n",
    "    })\n",
    ")\n",
    "\n",
    "# Filter out 'N.S.' and 'INS'\n",
    "mf_geo = mf_geo[\n",
    "    (mf_geo['Soil_depth'] != 'N.S.') &\n",
    "    (mf_geo['Total_Carbon'] != 'INS')\n",
    "]\n",
    "\n",
    "# Replace 'N.D.' with NaN\n",
    "mf_geo = mf_geo.replace('N.D.', np.nan)\n",
    "\n",
    "# Clean and convert metal columns\n",
    "metal_cols = ['Arsenic', 'Cadmium', 'Lead', 'Chromium']\n",
    "for col in metal_cols:\n",
    "    mf_geo[col] = (\n",
    "        mf_geo[col]\n",
    "        .astype(str)\n",
    "        .str.replace('<', '', regex=False)\n",
    "        .replace('', np.nan)\n",
    "        .astype(float)\n",
    "    )\n",
    "\n",
    "# Replace detection limit values with half\n",
    "detection_limits = {'Arsenic': 0.6, 'Cadmium': 0.1, 'Lead': 0.5, 'Chromium': 1.0}\n",
    "half_limits = {k: v/2 for k, v in detection_limits.items()}\n",
    "\n",
    "for col, limit in detection_limits.items():\n",
    "    mf_geo.loc[mf_geo[col] == limit, col] = half_limits[col]\n",
    "\n",
    "# Summarize missingness and stats\n",
    "summary_list = []\n",
    "for col in metal_cols:\n",
    "    summary_list.append({\n",
    "        'Element': col,\n",
    "        'N': len(mf_geo[col]),\n",
    "        'Missing': mf_geo[col].isna().sum(),\n",
    "        'Min': mf_geo[col].min(),\n",
    "        'Max': mf_geo[col].max()\n",
    "    })\n",
    "\n",
    "geo_sum = pd.DataFrame(summary_list).set_index('Element')\n",
    "print(geo_sum)\n",
    "\n",
    "# Option 1: Drop rows with any missing (like na.omit)\n",
    "newdata = mf_geo.dropna()\n",
    "print(f\"Rows after dropping all missing: {len(newdata)}\")\n",
    "\n",
    "# Option 2: Impute missing with mean\n",
    "mf_geo_new = mf_geo.copy()\n",
    "for col in metal_cols:\n",
    "    mf_geo_new[col] = mf_geo_new[col].fillna(mf_geo_new[col].mean())\n",
    "\n",
    "# Verify no missing in metals\n",
    "print(\"Missing after imputation:\")\n",
    "print(mf_geo_new[metal_cols].isna().sum())\n",
    "\n",
    "# Optional: Save cleaned data\n",
    "# mf_geo_new.to_csv(\"usa_geochemical_clean.csv\", index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "This Python tutorial mirrors the power of R’s `{dplyr}` and `{tidyr}` using **pandas**, arguably the most essential library for data manipulation in Python. You’ve learned how to:\n",
    "\n",
    "- Load, join, filter, select, rename, and relocate columns.\n",
    "- Create new variables, group data, and compute summaries.\n",
    "- Pivot data between long and wide formats.\n",
    "- Clean messy real-world datasets (handling missing values, detection limits, string cleaning).\n",
    "- Use method chaining for readable, efficient workflows.\n",
    "\n",
    "While Python doesn’t have a native pipe operator, **method chaining** and `.pipe()` provide equally powerful — and often more flexible — alternatives.\n",
    "\n",
    "As you advance, explore:\n",
    "- `polars` for faster DataFrame operations.\n",
    "- `pyjanitor` for even more R-like data cleaning verbs.\n",
    "- `siuba` for dplyr-style syntax in Python.\n",
    "- Integration with `scikit-learn` for modeling and `plotly`/`seaborn` for visualization.\n",
    "\n",
    "**Data wrangling is iterative and creative — practice with diverse datasets, and you’ll become fluent in transforming raw data into actionable insights.**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Resources\n",
    "\n",
    "1. [pandas User Guide](https://pandas.pydata.org/docs/user_guide/index.html)\n",
    "2. [Python for Data Analysis (Book by Wes McKinney)](https://wesmckinney.com/book/)\n",
    "3. [Real Python — pandas Tutorials](https://realpython.com/learning-paths/pandas-data-science/)\n",
    "4. [Kaggle pandas Course](https://www.kaggle.com/learn/pandas)\n",
    "5. [YouTube: Data Analysis with Python — freeCodeCamp](https://www.youtube.com/watch?v=r-uOLxNrNk8)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
