{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a href=\"https://github.com/zia207/python-colab/blob/main/NoteBook/Python_for_Beginners/001-03-02-data-wrangling-pyjanitor-python.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "![alt text](http://drive.google.com/uc?export=view&id=1IFEWet-Aw4DhkkVe1xv_2YYqlvRe9m5_)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 3.2 Data Wrangling with pyjanitor\n",
    "\n",
    "`pyjanitor` is a Python library that extends pandas, providing intuitive functions for data cleaning and preprocessing. Inspired by R's janitor package, it simplifies common data manipulation tasks with clear, chainable methods. Key features include cleaning column names, handling missing values, filtering, and grouping data, making it ideal for streamlining data preparation workflows. It integrates seamlessly with pandas DataFrames, offering a user-friendly API for both beginners and advanced users."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Prerequisites\n",
    "\n",
    "Install the required packages:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "import importlib.util\n",
    "import sys\n",
    "\n",
    "# List of required packages\n",
    "packages = ['pandas', 'janitor']\n",
    "\n",
    "# Check and install missing packages\n",
    "for package in packages:\n",
    "    if not importlib.util.find_spec(package):\n",
    "        try:\n",
    "            import pip\n",
    "            pip.main(['install', package])\n",
    "        except ImportError:\n",
    "            print(f\"Failed to install {package}. Pip is not available.\")\n",
    "\n",
    "\n",
    "# Import packages\n",
    "import pandas as pd\n",
    "import janitor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pandas installed: True\n",
      "janitor installed: True\n"
     ]
    }
   ],
   "source": [
    "# Verify package availability\n",
    "for package in packages:\n",
    "    print(f\"{package} installed: {bool(importlib.util.find_spec(package))}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Wrangling\n",
    "\n",
    "We will create some \"messy\" data and clean it using pyjanitor functions, including `clean_names()`, `remove_empty()`, `find_duplicates()`, and `drop_constant_columns()`. Later, we will clean a real dataset to demonstrate a complete workflow."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### clean_names()\n",
    "\n",
    "The `clean_names()` function standardizes column names by converting them to snake_case and removing special characters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Column One  Column Two!!  Column Three $  %Column four\n",
      "0           1             6              11            11\n",
      "1           2             7              12            12\n",
      "2           3             8              13            13\n",
      "3           4             9              14            14\n",
      "4           5            10              15            15\n",
      "\n",
      "DataFrame with cleaned names:\n",
      "   column_one  column_two!!  column_three_$  %column_four\n",
      "0           1             6              11            11\n",
      "1           2             7              12            12\n",
      "2           3             8              13            13\n",
      "3           4             9              14            14\n",
      "4           5            10              15            15\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with messy column names\n",
    "df = pd.DataFrame({\n",
    "    \"Column One\": [1, 2, 3, 4, 5],\n",
    "    \"Column Two!!\": [6, 7, 8, 9, 10],\n",
    "    \"Column Three $\": [11, 12, 13, 14, 15],\n",
    "    \"%Column four\": [11, 12, 13, 14, 15]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())\n",
    "\n",
    "# Clean column names\n",
    "df_cleaned = df.clean_names()\n",
    "print(\"\\nDataFrame with cleaned names:\")\n",
    "print(df_cleaned.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### get_dupes()\n",
    "\n",
    "The `get_dupes()` function identifies duplicate rows in a DataFrame based on specified columns."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Column One Column Two\n",
      "0           1          A\n",
      "1           2          B\n",
      "2           3          C\n",
      "3           1          A\n",
      "\n",
      "Duplicate rows:\n",
      "   Column One Column Two\n",
      "0           1          A\n",
      "3           1          A\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: columns is deprecated; use column_names\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with duplicate rows\n",
    "df = pd.DataFrame({\n",
    "    \"Column One\": [1, 2, 3, 1],\n",
    "    \"Column Two\": [\"A\", \"B\", \"C\", \"A\"]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Find duplicates using pyjanitor's get_dupes function\n",
    "duplicates = df.get_dupes(columns=[\"Column One\", \"Column Two\"])\n",
    "\n",
    "print(\"\\nDuplicate rows:\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### remove_empty()\n",
    "\n",
    "The  `remove_empty()` function removes rows or columns that are entirely empty or contain only missing values."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "     x    y     z\n",
      "0  1.0  NaN  None\n",
      "1  NaN  NaN  None\n",
      "2  4.0  3.0  None\n",
      "\n",
      "DataFrame after removing empty rows/columns:\n",
      "     x    y\n",
      "0  1.0  NaN\n",
      "1  4.0  3.0\n"
     ]
    }
   ],
   "source": [
    "# Create a DataFrame with empty rows and columns\n",
    "df = pd.DataFrame({\n",
    "    \"x\": [1, None, 4],\n",
    "    \"y\": [None, None, 3],\n",
    "    \"z\": [None, None, None]\n",
    "})\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df)\n",
    "\n",
    "# Remove empty rows and columns\n",
    "df_cleaned = df.remove_empty()\n",
    "print(\"\\nDataFrame after removing empty rows/columns:\")\n",
    "print(df_cleaned)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Cleaning Messy Data\n",
    "\n",
    "Now we will clean on very messy data using some functions of janitor packages. We will use [Lung Cancer Mortality data](https://www.dropbox.com/s/ovm7dc2szax6kcz/USA_LBC_Data.csv?dl=0).\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data\n",
    "\n",
    "The raw file has **one column** containing all fields separated by `|`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Original DataFrame:\n",
      "   Lung Cancer Moratlity Rates and Risk in USA, Data Provider: Zia Ahmed Unnamed: 1       Unnamed: 2      Unnamed: 3   Unnamed: 4   Unnamed: 5 Unnamed: 6      Unnamed: 7          Unnamed: 8 Unnamed: 9 Unnamed: 10 Unnamed: 11 Unnamed: 12 Unnamed: 13 Unnamed: 14 Unnamed: 15  Unnamed: 16 Unnamed: 17 Unnamed: 18 Unnamed: 19      Unnamed: 20 Unnamed: 21 Unnamed: 22       Unnamed: 23         Unnamed: 24      Unnamed: 25\n",
      "0                                          REGION_ID                          STATE           County  Empty Column 1            X            Y       Fips  Empty_Column 2  LCB Mortality Rate    Smoking      PM  25         NO2         SO2       Ozone      Pop 65   Pop Black  Pop Hipanic   Pop White   Education   Poverty %  Income Equality   Uninsured         DEM  Radon Zone Class         Urban Rural  Coal Production\n",
      "1                                                  3                        Alabama   Baldwin County             NaN  789777.5039  884557.0795       1003             NaN                48.1       20.8        7.89      0.7939    0.035343       39.79        19.5        9.24         4.54       83.06          66       13.14              4.5       13.34       36.78            Zone-3  Medium/small metro               No\n",
      "2                                                  3                        Alabama    Butler County             NaN  877731.5725   1007285.71       1013             NaN                38.3         26        8.46      0.6344      0.0135       38.31          19       43.94         1.26       52.64          38       26.14              5.1       12.74      111.70            Zone-3            Nonmetro               No\n",
      "3                                                  3                        Alabama    Butler County             NaN  877731.5725   1007285.71       1013             NaN                38.3         26        8.46      0.6344      0.0135       38.31          19       43.94         1.26       52.64          38       26.14              5.1       12.74      111.70            Zone-3            Nonmetro               No\n",
      "4                                                  3                        Alabama  Chambers County             NaN  984214.6861  1148648.711       1017             NaN                49.6       25.1        8.87      0.8442    0.048177        40.1        18.9       39.24         2.14       56.42          47       21.52              4.7       13.34      227.03            Zone-3            Nonmetro               No\n"
     ]
    }
   ],
   "source": [
    "# Load the dataset\n",
    "url = \"https://github.com/zia207/r-colab/raw/main/Data/R_Beginners/USA_LBC_Data_raw.csv\"\n",
    "df = pd.read_csv(url)\n",
    "\n",
    "print(\"Original DataFrame:\")\n",
    "print(df.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Promote Row to Column Names\n",
    "\n",
    "The dataset may have descriptive text in the first row instead of proper column headers. We can use row_to_names() to set the first row as column names and remove it from the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after row_to_names:\n",
      "  REGION_ID    STATE           County Empty Column 1            X            Y  Fips Empty_Column 2 LCB Mortality Rate Smoking PM  25     NO2       SO2  Ozone Pop 65 Pop Black Pop Hipanic Pop White Education Poverty % Income Equality Uninsured     DEM Radon Zone Class         Urban Rural Coal Production\n",
      "1         3  Alabama   Baldwin County            NaN  789777.5039  884557.0795  1003            NaN               48.1    20.8   7.89  0.7939  0.035343  39.79   19.5      9.24        4.54     83.06        66     13.14             4.5     13.34   36.78           Zone-3  Medium/small metro              No\n",
      "2         3  Alabama    Butler County            NaN  877731.5725   1007285.71  1013            NaN               38.3      26   8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "3         3  Alabama    Butler County            NaN  877731.5725   1007285.71  1013            NaN               38.3      26   8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "4         3  Alabama  Chambers County            NaN  984214.6861  1148648.711  1017            NaN               49.6    25.1   8.87  0.8442  0.048177   40.1   18.9     39.24        2.14     56.42        47     21.52             4.7     13.34  227.03           Zone-3            Nonmetro              No\n",
      "5       NaN      NaN              NaN            NaN          NaN          NaN   NaN            NaN                NaN     NaN    NaN     NaN       NaN    NaN    NaN       NaN         NaN       NaN       NaN       NaN             NaN       NaN     NaN              NaN                 NaN             NaN\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: row_number is deprecated; use row_numbers\n",
      "  warn(\n",
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: remove_row is deprecated; use remove_rows\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Promote first row to column names\n",
    "df_01 = df.row_to_names(row_number=0, remove_row=True, remove_rows_above=True)\n",
    "print(\"\\nDataFrame after row_to_names:\")\n",
    "print(df_01.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "### Remove Empty Rows and Columns\n",
    "\n",
    "Next, we remove any empty rows or columns using `remove_empty()`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame after removing empty rows/columns:\n",
      "  REGION_ID    STATE           County            X            Y  Fips LCB Mortality Rate Smoking PM  25     NO2       SO2  Ozone Pop 65 Pop Black Pop Hipanic Pop White Education Poverty % Income Equality Uninsured     DEM Radon Zone Class         Urban Rural Coal Production\n",
      "0         3  Alabama   Baldwin County  789777.5039  884557.0795  1003               48.1    20.8   7.89  0.7939  0.035343  39.79   19.5      9.24        4.54     83.06        66     13.14             4.5     13.34   36.78           Zone-3  Medium/small metro              No\n",
      "1         3  Alabama    Butler County  877731.5725   1007285.71  1013               38.3      26   8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "2         3  Alabama    Butler County  877731.5725   1007285.71  1013               38.3      26   8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "3         3  Alabama  Chambers County  984214.6861  1148648.711  1017               49.6    25.1   8.87  0.8442  0.048177   40.1   18.9     39.24        2.14     56.42        47     21.52             4.7     13.34  227.03           Zone-3            Nonmetro              No\n",
      "4         3  Alabama   Choctaw County  726606.4744  1023615.755  1023               31.8    21.8   8.58  0.5934  0.023989  37.07   22.1     41.94        0.86     56.28        55     23.06             5.8     12.86   68.24           Zone-3            Nonmetro              No\n"
     ]
    }
   ],
   "source": [
    "# Remove empty rows and columns\n",
    "df_02 = df_01.remove_empty()\n",
    "print(\"\\nDataFrame after removing empty rows/columns:\")\n",
    "print(df_02.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean Column Names\n",
    "\n",
    "We standardize column names using clean_names()."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with cleaned column names:\n",
      "  region_id    state           county            x            y  fips lcb_mortality_rate smoking pm_25     no2       so2  ozone pop_65 pop_black pop_hipanic pop_white education poverty_% income_equality uninsured     dem radon_zone_class         urban_rural coal_production\n",
      "0         3  Alabama   Baldwin County  789777.5039  884557.0795  1003               48.1    20.8  7.89  0.7939  0.035343  39.79   19.5      9.24        4.54     83.06        66     13.14             4.5     13.34   36.78           Zone-3  Medium/small metro              No\n",
      "1         3  Alabama    Butler County  877731.5725   1007285.71  1013               38.3      26  8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "2         3  Alabama    Butler County  877731.5725   1007285.71  1013               38.3      26  8.46  0.6344    0.0135  38.31     19     43.94        1.26     52.64        38     26.14             5.1     12.74  111.70           Zone-3            Nonmetro              No\n",
      "3         3  Alabama  Chambers County  984214.6861  1148648.711  1017               49.6    25.1  8.87  0.8442  0.048177   40.1   18.9     39.24        2.14     56.42        47     21.52             4.7     13.34  227.03           Zone-3            Nonmetro              No\n",
      "4         3  Alabama   Choctaw County  726606.4744  1023615.755  1023               31.8    21.8  8.58  0.5934  0.023989  37.07   22.1     41.94        0.86     56.28        55     23.06             5.8     12.86   68.24           Zone-3            Nonmetro              No\n"
     ]
    }
   ],
   "source": [
    "# Clean column names\n",
    "df_03 = df_02.clean_names()\n",
    "print(\"\\nDataFrame with cleaned column names:\")\n",
    "print(df_03.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Convert Data Types\n",
    "\n",
    "The dataset may have incorrect data types (e.g., numeric columns stored as strings). We use `convert_to_numeric()` and pandas' `astype()` to fix this. Assuming columns 4 to 21 are numeric and 22 to 24 are categorical "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "DataFrame with corrected data types:\n",
      "region_id               object\n",
      "state                   object\n",
      "county                  object\n",
      "x                      float64\n",
      "y                      float64\n",
      "fips                     int64\n",
      "lcb_mortality_rate     float64\n",
      "smoking                float64\n",
      "pm_25                  float64\n",
      "no2                    float64\n",
      "so2                    float64\n",
      "ozone                  float64\n",
      "pop_65                 float64\n",
      "pop_black              float64\n",
      "pop_hipanic            float64\n",
      "pop_white              float64\n",
      "education                int64\n",
      "poverty_%              float64\n",
      "income_equality        float64\n",
      "uninsured              float64\n",
      "dem                    float64\n",
      "radon_zone_class      category\n",
      "urban_rural           category\n",
      "coal_production       category\n",
      "dtype: object\n"
     ]
    }
   ],
   "source": [
    "# Assuming df_03 is the DataFrame after cleaning names\n",
    "# Convert columns 4 to 21 to numeric using pandas' pd.to_numeric\n",
    "# Convert columns 22 to 24 to categorical\n",
    "df_04 = df_03.copy()  # Create a copy to avoid modifying the original\n",
    "for col in df_03.columns[3:21]:\n",
    "    df_04[col] = pd.to_numeric(df_03[col], errors=\"coerce\")\n",
    "df_04 = df_04.astype({col: 'category' for col in df_03.columns[21:24]})\n",
    "\n",
    "print(\"\\nDataFrame with corrected data types:\")\n",
    "print(df_04.dtypes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Remove Duplicates\n",
    "\n",
    "We check for and remove duplicate rows based on a key column (e.g., fips)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Duplicate rows based on 'fips':\n",
      "   region_id     state           county            x             y  fips  lcb_mortality_rate  smoking  pm_25     no2       so2  ozone  pop_65  pop_black  pop_hipanic  pop_white  education  poverty_%  income_equality  uninsured     dem radon_zone_class urban_rural coal_production\n",
      "1          3   Alabama    Butler County  877731.5725  1.007286e+06  1013                38.3     26.0   8.46  0.6344  0.013500  38.31    19.0      43.94         1.26      52.64         38      26.14              5.1      12.74  111.70           Zone-3    Nonmetro              No\n",
      "2          3   Alabama    Butler County  877731.5725  1.007286e+06  1013                38.3     26.0   8.46  0.6344  0.013500  38.31    19.0      43.94         1.26      52.64         38      26.14              5.1      12.74  111.70           Zone-3    Nonmetro              No\n",
      "11         3   Alabama  Escambia County  839009.4497  9.335802e+05  1053                58.3     25.3   8.08  0.5742  0.025900  38.03    17.3      32.00         2.16      60.42         38      25.70              5.6      15.64   72.04           Zone-3    Nonmetro              No\n",
      "12         3   Alabama  Escambia County  839009.4497  9.335802e+05  1053                58.3     25.3   8.08  0.5742  0.025900  38.03    17.3      32.00         2.16      60.42         38      25.70              5.6      15.64   72.04           Zone-3    Nonmetro              No\n",
      "25         3  Arkansas   Bradley County  354204.4236  1.162472e+06  5011                69.9     25.0   8.32  0.5579  0.009467  37.67    18.4      27.36        14.74      56.00         46      26.32              5.8      16.12   56.66           Zone-3    Nonmetro              No\n",
      "26         3  Arkansas   Bradley County  354204.4236  1.162472e+06  5011                69.9     25.0   8.32  0.5579  0.009467  37.67    18.4      27.36        14.74      56.00         46      26.32              5.8      16.12   56.66           Zone-3    Nonmetro              No\n",
      "\n",
      "DataFrame after removing duplicates:\n",
      "  region_id    state           county            x             y  fips  lcb_mortality_rate  smoking  pm_25     no2       so2  ozone  pop_65  pop_black  pop_hipanic  pop_white  education  poverty_%  income_equality  uninsured     dem radon_zone_class         urban_rural coal_production\n",
      "0         3  Alabama   Baldwin County  789777.5039  8.845571e+05  1003                48.1     20.8   7.89  0.7939  0.035343  39.79    19.5       9.24         4.54      83.06         66      13.14              4.5      13.34   36.78           Zone-3  Medium/small metro              No\n",
      "1         3  Alabama    Butler County  877731.5725  1.007286e+06  1013                38.3     26.0   8.46  0.6344  0.013500  38.31    19.0      43.94         1.26      52.64         38      26.14              5.1      12.74  111.70           Zone-3            Nonmetro              No\n",
      "3         3  Alabama  Chambers County  984214.6861  1.148649e+06  1017                49.6     25.1   8.87  0.8442  0.048177  40.10    18.9      39.24         2.14      56.42         47      21.52              4.7      13.34  227.03           Zone-3            Nonmetro              No\n",
      "4         3  Alabama   Choctaw County  726606.4744  1.023616e+06  1023                31.8     21.8   8.58  0.5934  0.023989  37.07    22.1      41.94         0.86      56.28         55      23.06              5.8      12.86   68.24           Zone-3            Nonmetro              No\n",
      "5         3  Alabama    Clarke County  770408.8693  9.889105e+05  1025                42.0     22.6   8.42  0.6432  0.033700  37.68    19.0      43.96         1.34      52.98         39      24.60              8.2      13.28   69.29           Zone-3            Nonmetro              No\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: columns is deprecated; use column_names\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "\n",
    "# Check for duplicates using pyjanitor's get_dupes\n",
    "duplicates = df_04.get_dupes(columns=\"fips\")\n",
    "print(\"\\nDuplicate rows based on 'fips':\")\n",
    "print(duplicates)\n",
    "\n",
    "# Remove duplicates using pandas' drop_duplicates\n",
    "df_05 = df_04.drop_duplicates(subset=\"fips\", keep=\"first\")\n",
    "print(\"\\nDataFrame after removing duplicates:\")\n",
    "print(df_05.head())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Combine All Steps\n",
    "\n",
    "We can chain all operations using pandas' method chaining for a clean workflow."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Fully cleaned DataFrame:\n",
      "  region_id    state           county            x             y  fips  lcb_mortality_rate  smoking  pm_25     no2       so2  ozone  pop_65  pop_black  pop_hipanic  pop_white  education  poverty_%  income_equality  uninsured     dem radon_zone_class         urban_rural coal_production\n",
      "0         3  Alabama   Baldwin County  789777.5039  8.845571e+05  1003                48.1     20.8   7.89  0.7939  0.035343  39.79    19.5       9.24         4.54      83.06         66      13.14              4.5      13.34   36.78           Zone-3  Medium/small metro              No\n",
      "1         3  Alabama    Butler County  877731.5725  1.007286e+06  1013                38.3     26.0   8.46  0.6344  0.013500  38.31    19.0      43.94         1.26      52.64         38      26.14              5.1      12.74  111.70           Zone-3            Nonmetro              No\n",
      "3         3  Alabama  Chambers County  984214.6861  1.148649e+06  1017                49.6     25.1   8.87  0.8442  0.048177  40.10    18.9      39.24         2.14      56.42         47      21.52              4.7      13.34  227.03           Zone-3            Nonmetro              No\n",
      "4         3  Alabama   Choctaw County  726606.4744  1.023616e+06  1023                31.8     21.8   8.58  0.5934  0.023989  37.07    22.1      41.94         0.86      56.28         55      23.06              5.8      12.86   68.24           Zone-3            Nonmetro              No\n",
      "5         3  Alabama    Clarke County  770408.8693  9.889105e+05  1025                42.0     22.6   8.42  0.6432  0.033700  37.68    19.0      43.96         1.34      52.98         39      24.60              8.2      13.28   69.29           Zone-3            Nonmetro              No\n",
      "\n",
      "Data types:\n",
      "region_id               object\n",
      "state                   object\n",
      "county                  object\n",
      "x                      float64\n",
      "y                      float64\n",
      "fips                     int64\n",
      "lcb_mortality_rate     float64\n",
      "smoking                float64\n",
      "pm_25                  float64\n",
      "no2                    float64\n",
      "so2                    float64\n",
      "ozone                  float64\n",
      "pop_65                 float64\n",
      "pop_black              float64\n",
      "pop_hipanic            float64\n",
      "pop_white              float64\n",
      "education                int64\n",
      "poverty_%              float64\n",
      "income_equality        float64\n",
      "uninsured              float64\n",
      "dem                    float64\n",
      "radon_zone_class      category\n",
      "urban_rural           category\n",
      "coal_production       category\n",
      "dtype: object\n",
      "\n",
      "Duplicate rows based on 'fips':\n",
      "Empty DataFrame\n",
      "Columns: [region_id, state, county, x, y, fips, lcb_mortality_rate, smoking, pm_25, no2, so2, ozone, pop_65, pop_black, pop_hipanic, pop_white, education, poverty_%, income_equality, uninsured, dem, radon_zone_class, urban_rural, coal_production]\n",
      "Index: []\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: row_number is deprecated; use row_numbers\n",
      "  warn(\n",
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: remove_row is deprecated; use remove_rows\n",
      "  warn(\n",
      "/home/zia207/.local/lib/python3.10/site-packages/janitor/utils.py:367: DeprecationWarning: columns is deprecated; use column_names\n",
      "  warn(\n"
     ]
    }
   ],
   "source": [
    "# Cleaning pipeline\n",
    "df_clean = df.copy()\n",
    "df_clean = (df_clean\n",
    "            .row_to_names(row_number=0, remove_row=True, remove_rows_above=True)\n",
    "            .remove_empty()\n",
    "            .clean_names()\n",
    ")\n",
    "\n",
    "# Convert columns 4 to 21 to numeric\n",
    "numeric_cols = df_clean.columns[3:21]\n",
    "for col in numeric_cols:\n",
    "    df_clean[col] = pd.to_numeric(df_clean[col], errors=\"coerce\")\n",
    "\n",
    "# Convert columns 22 to 24 to categorical and remove duplicates\n",
    "df_clean = (df_clean\n",
    "            .astype({col: 'category' for col in df_clean.columns[21:24]})\n",
    "            .drop_duplicates(subset=\"fips\", keep=\"first\")\n",
    ")\n",
    "\n",
    "print(\"\\nFully cleaned DataFrame:\")\n",
    "print(df_clean.head())\n",
    "print(\"\\nData types:\")\n",
    "print(df_clean.dtypes)\n",
    "\n",
    "# Check for duplicates\n",
    "duplicates = df_clean.get_dupes(columns=\"fips\")\n",
    "print(\"\\nDuplicate rows based on 'fips':\")\n",
    "print(duplicates)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Some Important Functions"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **`conditional_join`**\n",
    "\n",
    "Unlike `pd.merge`, which only handles equi-joins (equality conditions), `conditional_join` supports joins based on inequality operators (`<`, `<=`, `>`, `>=`, `==`, `!=`) or combinations of both.\n",
    "\n",
    ">df.conditional_join(right_df, (left_column, right_column, operator), ...)\n",
    "\n",
    "Where:\n",
    "\n",
    "-   `df`: The left DataFrame\n",
    "\n",
    "-   `right_df`: The right DataFrame to join with\n",
    "\n",
    "-   `(left_column, right_column, operator)`: A tuple defining the join condition\n",
    "\n",
    "-   `operator`: One of `==`, `!=`, `<=`, `<`, `>=`, `>`"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Range Join**\n",
    "\n",
    "Let's say we have two datasets:\n",
    "\n",
    "-   `employees`: Contains employee IDs and their salary ranges\n",
    "\n",
    "-   `jobs`: Contains job positions with required salary ranges\n",
    "\n",
    "We want to find which jobs each employee qualifies for based on their salary falling within the job's range."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Employees:\n",
      "   employee_id     name  salary\n",
      "0            1    Alice   50000\n",
      "1            2      Bob   75000\n",
      "2            3  Charlie   90000\n",
      "3            4    Diana   60000\n",
      "\n",
      "Jobs:\n",
      "          job_title  min_salary  max_salary\n",
      "0  Junior Developer       40000       60000\n",
      "1  Senior Developer       60000       85000\n",
      "2           Manager       80000      120000\n",
      "3          Director      100000      150000\n"
     ]
    }
   ],
   "source": [
    "# Create sample data\n",
    "employees = pd.DataFrame({\n",
    "    'employee_id': [1, 2, 3, 4],\n",
    "    'name': ['Alice', 'Bob', 'Charlie', 'Diana'],\n",
    "    'salary': [50000, 75000, 90000, 60000]\n",
    "})\n",
    "\n",
    "jobs = pd.DataFrame({\n",
    "    'job_title': ['Junior Developer', 'Senior Developer', 'Manager', 'Director'],\n",
    "    'min_salary': [40000, 60000, 80000, 100000],\n",
    "    'max_salary': [60000, 85000, 120000, 150000]\n",
    "})\n",
    "\n",
    "print(\"Employees:\")\n",
    "print(employees)\n",
    "print(\"\\nJobs:\")\n",
    "print(jobs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Employees qualified for jobs:\n",
      "   employee_id     name  salary         job_title  min_salary  max_salary\n",
      "0            1    Alice   50000  Junior Developer       40000       60000\n",
      "1            2      Bob   75000  Senior Developer       60000       85000\n",
      "2            3  Charlie   90000           Manager       80000      120000\n",
      "3            4    Diana   60000  Junior Developer       40000       60000\n",
      "4            4    Diana   60000  Senior Developer       60000       85000\n"
     ]
    }
   ],
   "source": [
    "# Join employees with jobs where employee salary is between min and max salary\n",
    "result = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='),  # employee.salary >= job.min_salary\n",
    "    ('salary', 'max_salary', '<=')   # employee.salary <= job.max_salary\n",
    ")\n",
    "\n",
    "print(\"\\nEmployees qualified for jobs:\")\n",
    "print(result)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Controlling Output Columns**\n",
    "\n",
    "Often, you don't need all columns from both DataFrames. Use `df_columns` and `right_columns` to select specific ones:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Simplified result - only employee info and job title:\n",
      "   employee_id     name  salary         job_title\n",
      "0            1    Alice   50000  Junior Developer\n",
      "1            2      Bob   75000  Senior Developer\n",
      "2            3  Charlie   90000           Manager\n",
      "3            4    Diana   60000  Junior Developer\n",
      "4            4    Diana   60000  Senior Developer\n"
     ]
    }
   ],
   "source": [
    "# Only keep employee info and job title\n",
    "result_slim = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    df_columns=['employee_id', 'name', 'salary'],  # Columns from employees\n",
    "    right_columns='job_title'  # Only job_title from jobs\n",
    ")\n",
    "\n",
    "print(\"\\nSimplified result - only employee info and job title:\")\n",
    "print(result_slim)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### **Using Different Join Types**\n",
    "\n",
    "You can specify different types of joins similar to `pd.merge`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Left join - all employees included:\n",
      "   employee_id     name  salary         job_title  min_salary  max_salary\n",
      "0            1    Alice   50000  Junior Developer       40000       60000\n",
      "1            2      Bob   75000  Senior Developer       60000       85000\n",
      "2            3  Charlie   90000           Manager       80000      120000\n",
      "3            4    Diana   60000  Junior Developer       40000       60000\n",
      "4            4    Diana   60000  Senior Developer       60000       85000\n"
     ]
    }
   ],
   "source": [
    "# Left join: Include all employees, even if they don't qualify for any job\n",
    "result_left = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    how='left'\n",
    ")\n",
    "\n",
    "print(\"\\nLeft join - all employees included:\")\n",
    "print(result_left)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Getting First or Last Match Only\n",
    "When multiple matches exist (like Charlie qualifying for two jobs), you might only want one:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "First match only:\n",
      "   employee_id     name  salary         job_title  min_salary  max_salary\n",
      "0            1    Alice   50000  Junior Developer       40000       60000\n",
      "1            2      Bob   75000  Senior Developer       60000       85000\n",
      "2            3  Charlie   90000           Manager       80000      120000\n",
      "3            4    Diana   60000  Junior Developer       40000       60000\n",
      "\n",
      "Last match only:\n",
      "   employee_id     name  salary         job_title  min_salary  max_salary\n",
      "0            1    Alice   50000  Junior Developer       40000       60000\n",
      "1            2      Bob   75000  Senior Developer       60000       85000\n",
      "2            3  Charlie   90000           Manager       80000      120000\n",
      "3            4    Diana   60000  Senior Developer       60000       85000\n"
     ]
    }
   ],
   "source": [
    "# Get only the first matching job for each employee\n",
    "result_first = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    keep='first'\n",
    ")\n",
    "\n",
    "print(\"\\nFirst match only:\")\n",
    "print(result_first)\n",
    "\n",
    "# Get only the last matching job for each employee\n",
    "result_last = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    keep='last'\n",
    ")\n",
    "\n",
    "print(\"\\nLast match only:\")\n",
    "print(result_last)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adding an Indicator Column\n",
    "To understand the source of each row (especially useful in outer joins):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Outer join with indicator column:\n",
      "   employee_id     name   salary         job_title  min_salary  max_salary  \\\n",
      "0          1.0    Alice  50000.0  Junior Developer       40000       60000   \n",
      "1          2.0      Bob  75000.0  Senior Developer       60000       85000   \n",
      "2          3.0  Charlie  90000.0           Manager       80000      120000   \n",
      "3          4.0    Diana  60000.0  Junior Developer       40000       60000   \n",
      "4          4.0    Diana  60000.0  Senior Developer       60000       85000   \n",
      "5          NaN      NaN      NaN          Director      100000      150000   \n",
      "\n",
      "       _merge  \n",
      "0        both  \n",
      "1        both  \n",
      "2        both  \n",
      "3        both  \n",
      "4        both  \n",
      "5  right_only  \n"
     ]
    }
   ],
   "source": [
    "# Outer join with indicator\n",
    "result_outer = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    how='outer',\n",
    "    indicator=True\n",
    ")\n",
    "\n",
    "print(\"\\nOuter join with indicator column:\")\n",
    "print(result_outer)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Performance Optimization with Numba\n",
    "For large datasets, you can significantly speed up computations by enabling Numba (if installed):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Enable Numba optimization (requires numba package)\n",
    "result_optimized = employees.conditional_join(\n",
    "    jobs,\n",
    "    ('salary', 'min_salary', '>='), \n",
    "    ('salary', 'max_salary', '<='),\n",
    "    use_numba=True\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "###  Date Range Analysis\n",
    "Here's another practical example: finding events that occurred during specific time periods."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Website visits matched with campaigns:\n",
      "   user_id          visit_time    campaign_name start_date   end_date\n",
      "0        1 2023-01-15 10:00:00      Winter Sale 2023-01-10 2023-01-25\n",
      "1        1 2023-01-15 10:00:00   New Year Promo 2023-01-15 2023-01-31\n",
      "2        2 2023-01-20 14:30:00      Winter Sale 2023-01-10 2023-01-25\n",
      "3        2 2023-01-20 14:30:00   New Year Promo 2023-01-15 2023-01-31\n",
      "4        3 2023-02-05 09:15:00  February Frenzy 2023-02-01 2023-02-28\n",
      "5        4 2023-02-10 16:20:00  February Frenzy 2023-02-01 2023-02-28\n",
      "6        5 2023-02-25 11:45:00  February Frenzy 2023-02-01 2023-02-28\n"
     ]
    }
   ],
   "source": [
    "# Sample  website visits and marketing campaigns\n",
    "visits = pd.DataFrame({\n",
    "    'user_id': [1, 2, 3, 4, 5],\n",
    "    'visit_time': pd.to_datetime([\n",
    "        '2023-01-15 10:00:00',\n",
    "        '2023-01-20 14:30:00',\n",
    "        '2023-02-05 09:15:00',\n",
    "        '2023-02-10 16:20:00',\n",
    "        '2023-02-25 11:45:00'\n",
    "    ])\n",
    "})\n",
    "\n",
    "campaigns = pd.DataFrame({\n",
    "    'campaign_name': ['Winter Sale', 'New Year Promo', 'February Frenzy'],\n",
    "    'start_date': pd.to_datetime(['2023-01-10', '2023-01-15', '2023-02-01']),\n",
    "    'end_date': pd.to_datetime(['2023-01-25', '2023-01-31', '2023-02-28'])\n",
    "})\n",
    "\n",
    "# Find which campaign each visit occurred during\n",
    "visit_campaigns = visits.conditional_join(\n",
    "    campaigns,\n",
    "    ('visit_time', 'start_date', '>='),  # visit_time >= campaign_start\n",
    "    ('visit_time', 'end_date', '<=')     # visit_time <= campaign_end\n",
    ")\n",
    "\n",
    "print(\"\\nWebsite visits matched with campaigns:\")\n",
    "print(visit_campaigns)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Summary and Conclusion\n",
    "\n",
    "In this notebook, we explored data wrangling using the `pyjanitor` library in Python. We covered key functions such as `clean_names()`, `get_dupes()`, and `remove_empty()`, demonstrating how they can simplify common data cleaning tasks. By applying these functions to a real-world dataset, we illustrated a step-by-step approach to transforming messy data into a tidy format. The integration of `pyjanitor` with pandas allows for intuitive and efficient data manipulation, making it a valuable tool for data scientists and analysts. Overall, `pyjanitor` enhances the data cleaning process, enabling users to focus more on analysis and insights."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Resources\n",
    "\n",
    "- [pyjanitor Documentation](https://pyjanitor.readthedocs.io/)\n",
    "- [pandas CROSSTAB Docs](https://pandas.pydata.org/docs/reference/api/pandas.crosstab.html)\n",
    "- [R janitor Vignette](https://cran.r-project.org/web/packages/janitor/vignettes/janitor.html)\n",
    "- [Tidyverse Style Guide](https://style.tidyverse.org/) â†’ Applies to Python too!"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
